[["Map",1,2,9,10,2022,2023],"meta::meta",["Map",3,4,5,6,7,8],"astro-version","5.12.0","content-config-digest","b8d85079526fceeb","astro-config-digest","{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"site\":\"https://ur-rad.github.io/\",\"compressHTML\":true,\"base\":\"/\",\"trailingSlash\":\"ignore\",\"output\":\"static\",\"scopedStyleStrategy\":\"attribute\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"_astro\",\"serverEntry\":\"entry.mjs\",\"redirects\":true,\"inlineStylesheets\":\"auto\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":true,\"port\":4321,\"streaming\":true,\"allowedHosts\":[]},\"redirects\":{\"/fss_2025/\":\"/\"},\"prefetch\":true,\"image\":{\"endpoint\":{\"route\":\"/_image\"},\"service\":{\"entrypoint\":\"astro/assets/services/sharp\",\"config\":{}},\"domains\":[\"webmention.io\"],\"remotePatterns\":[],\"responsiveStyles\":false},\"devToolbar\":{\"enabled\":true},\"markdown\":{\"syntaxHighlight\":false,\"shikiConfig\":{\"langs\":[],\"langAlias\":{},\"theme\":\"github-dark\",\"themes\":{},\"wrap\":false,\"transformers\":[]},\"remarkPlugins\":[null,null,null],\"rehypePlugins\":[[null,{\"rel\":[\"nofollow\",\"noreferrer\"],\"target\":\"_blank\"}],[null,{\"theme\":{\"light\":\"rose-pine-dawn\",\"dark\":\"rose-pine\"},\"transformers\":[{\"name\":\"@shikijs/transformers:notation-diff\"},{\"name\":\"@shikijs/transformers:meta-highlight\"}]}],null],\"remarkRehype\":{\"footnoteLabelProperties\":{\"className\":[\"\"]},\"footnoteBackContent\":\"⤴\"},\"gfm\":true,\"smartypants\":true},\"security\":{\"checkOrigin\":true},\"env\":{\"schema\":{\"WEBMENTION_API_KEY\":{\"context\":\"server\",\"access\":\"secret\",\"optional\":true,\"type\":\"string\"},\"WEBMENTION_URL\":{\"context\":\"client\",\"access\":\"public\",\"optional\":true,\"type\":\"string\"},\"WEBMENTION_PINGBACK\":{\"context\":\"client\",\"access\":\"public\",\"optional\":true,\"type\":\"string\"}},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":false,\"headingIdCompat\":false,\"preserveScriptOrder\":false,\"liveContentCollections\":false,\"csp\":false,\"rawEnvValues\":false},\"legacy\":{\"collections\":false}}","post",["Map",11,12,66,67,118,119,175,176,207,208,248,249,302,303,344,345,393,394,442,443,483,484,516,517,559,560,625,626,671,672,710,711,762,763,814,815,855,856,902,903,986,987,1034,1035,1066,1067,1119,1120,1170,1171,1206,1207,1243,1244,1306,1307,1338,1339,1383,1384,1446,1447,1517,1518,1578,1579,1608,1609,1654,1655,1703,1704,1757,1758,1787,1788,1923,1924,1963,1964],"4d-robot-navigation-relativistic-image-processing",{"id":11,"data":13,"filePath":47,"digest":48,"rendered":49},{"title":14,"abstract":15,"authors":16,"affiliations":25,"pdfUrl":28,"draft":29,"tags":30,"publishDate":46},"4D-based Robot Navigation using Relativistic Image Processing","Machine perception is an important prerequisite for safe interaction and locomotion in dynamic environments. This requires not only the timely perception of surrounding geometries and distances but also the ability to react to changing situations through predefined, learned but also reusable skill endings of a robot so that physical damage or bodily harm can be avoided. In this context, 4D perception offers the possibility of predicting one’s own position and changes in the environment over time. In this paper, we present a 4D-based approach to robot navigation using relativistic image processing. Relativistic image processing handles the temporal-related sensor information in a tensor model within a constructive 4D space. 4D-based navigation expands the causal understanding and the resulting interaction radius of a robot through the use of visual and sensory 4D information.",[17,21],{"name":18,"affiliationIndices":19},"Simone Müller",[20],0,{"name":22,"affiliationIndices":23},"Dieter Kranzlmüller",[24],1,[26,27],"Leibniz Supercomputing Centre","Ludwig-Maximilians-Universität München","https://drive.google.com/file/d/1ES1qAIwZu0K3I5hTfdspjSIXqd5cxYVr/view",false,[31,32,33,34,35,36,37,38,39,40,41,42,43,44,45],"4d-navigation","relativistic-image-processing","robot-navigation","machine-perception","temporal-perception","dynamic-environments","computer-vision","sensor-fusion","predictive-navigation","4d-space","tensor-models","visual-perception","causal-understanding","safe-navigation","robotics",["Date","2024-11-07T16:00:00.000Z"],"src/content/post/4d-robot-navigation-relativistic-image-processing.md","a2243fa3cc56fe19",{"html":50,"metadata":51},"",{"headings":52,"localImagePaths":53,"remoteImagePaths":54,"frontmatter":55,"imagePaths":65},[],[],[],{"title":14,"abstract":15,"authors":56,"affiliations":61,"tags":62,"publishDate":63,"draft":29,"pdfUrl":28,"readingTime":64},[57,59],{"name":18,"affiliationIndices":58},[20],{"name":22,"affiliationIndices":60},[24],[26,27],[31,32,33,34,35,36,37,38,39,40,41,42,43,44,45],"2024-11-07T16:00:00Z","0 min read",[],"birds-feather-animal-human-machine",{"id":66,"data":68,"filePath":97,"digest":98,"rendered":99},{"title":69,"abstract":70,"authors":71,"affiliations":86,"pdfUrl":91,"draft":29,"tags":92,"publishDate":96},"Birds of a Different Feather Flock Together: Exploring Opportunities and Challenges in Animal-Human-Machine Teaming","Animal-Human-Machine (AHM) teams are a type of hybrid intelligence system wherein interactions between a human, AI-enabled machine, and animal members can result in unique capabilities greater than the sum of their parts. This paper calls for a systematic approach to studying the design of AHM team structures to optimize performance and overcome limitations in various applied settings. We consider the challenges and opportunities in investigating the synergistic potential of AHM team members by introducing a set of dimensions of AHM team functioning to effectively utilize each member’s strengths while compensating for individual weaknesses. Using three representative examples of such teams— security screening, search-and-rescue, and guide dogs—the paper illustrates how AHM teams can tackle complex tasks. We conclude with open research directions that this multidimensional approach presents for studying hybrid human-AI systems beyond AHM teams.",[72,75,78,82],{"name":73,"affiliationIndices":74},"Myke C. Cohen",[20,24],{"name":76,"affiliationIndices":77},"Xiaoyun Yin",[20],{"name":79,"affiliationIndices":80},"David A. Grimm",[20,81],2,{"name":83,"affiliationIndices":84},"Reuth Mirsky",[85],3,[87,88,89,90],"Arizona State University","Aptima, Inc.","Georgia Institute of Technology","Tufts University","https://drive.google.com/file/d/1cL21sIKUt43dKoaiBBKulp_nZq_CV3P9/view",[93,94,95],"sally-anne","agents","conceptual-decomposition",["Date","2025-11-07T11:00:00.000Z"],"src/content/post/birds-feather-animal-human-machine.md","ced3a59bc359deaa",{"html":50,"metadata":100},{"headings":101,"localImagePaths":102,"remoteImagePaths":103,"frontmatter":104,"imagePaths":117},[],[],[],{"title":69,"abstract":70,"authors":105,"affiliations":114,"tags":115,"publishDate":116,"draft":29,"pdfUrl":91,"readingTime":64},[106,108,110,112],{"name":73,"affiliationIndices":107},[20,24],{"name":76,"affiliationIndices":109},[20],{"name":79,"affiliationIndices":111},[20,81],{"name":83,"affiliationIndices":113},[85],[87,88,89,90],[93,94,95],"2025-11-07T11:00:00Z",[],"categorical-representation-language",{"id":118,"data":120,"filePath":152,"digest":153,"rendered":154},{"title":121,"abstract":122,"authors":123,"affiliations":139,"pdfUrl":144,"draft":29,"tags":145,"publishDate":151},"A Categorical Representation Language and Computational System for Knowledge-Based Robotic Task Planning","Classical planning representation languages based on first-order logic have preliminarily been used to model and solve robotic task planning problems. Wider adoption of these representation languages, however, is hindered by the limitations present when managing implicit world changes with concise action models. To address this problem, we propose an alternative approach to representing and managing updates to world states during planning. Based on the category-theoretic concepts of C-sets and double-pushout rewriting (DPO), our proposed representation can effectively handle structured knowledge about world states that support domain abstractions at all levels. It formalizes the semantics of predicates according to a user-provided ontology and preserves the semantics when transitioning between world states. This method provides a formal semantics for using knowledge graphs and relational databases to model world states and updates in planning. In this paper, we conceptually compare our category-theoretic representation with the classical planning representation. We show that our proposed representation has advantages over the classical representation in terms of handling implicit preconditions and effects, and provides a more structured framework in which to model and solve planning problems.",[124,127,130,133,136],{"name":125,"affiliationIndices":126},"Angeline Aguinaldo",[20,24],{"name":128,"affiliationIndices":129},"Evan Patterson",[81],{"name":131,"affiliationIndices":132},"James Fairbanks",[85],{"name":134,"affiliationIndices":135},"William Regli",[20],{"name":137,"affiliationIndices":138},"Jaime Ruiz",[85],[140,141,142,143],"University of Maryland, College Park","Johns Hopkins University Applied Physics Laboratory","Topos Institute","University of Florida","https://drive.google.com/file/d/164vE3sixsiWR1AIY5rysK8T0ZfcJ--NH/view",[146,147,148,45,149,150],"categorical-representations","task-planning","knowledge-representation","category-theory","computational-systems",["Date","2023-10-26T10:00:00.000Z"],"src/content/post/categorical-representation-language.md","0be0846d43093801",{"html":50,"metadata":155},{"headings":156,"localImagePaths":157,"remoteImagePaths":158,"frontmatter":159,"imagePaths":174},[],[],[],{"title":121,"abstract":122,"authors":160,"affiliations":171,"tags":172,"publishDate":173,"draft":29,"pdfUrl":144,"readingTime":64},[161,163,165,167,169],{"name":125,"affiliationIndices":162},[20,24],{"name":128,"affiliationIndices":164},[81],{"name":131,"affiliationIndices":166},[85],{"name":134,"affiliationIndices":168},[20],{"name":137,"affiliationIndices":170},[85],[140,141,142,143],[146,147,148,45,149,150],"2023-10-26T10:00:00Z",[],"collision-control-multiple-autonomous-robots",{"id":175,"data":177,"filePath":193,"digest":194,"rendered":195},{"title":178,"abstract":179,"authors":180,"affiliations":184,"pdfUrl":185,"draft":29,"tags":186,"publishDate":192},"A Discussion on the Collision Control of Multiple Autonomous Robots","This paper discusses control methods for multi-agent autonomous robots subject to collision avoidance and unknown external forcing. Emphasis is focused on presenting control approaches based purely on stabilizing the active constraints that the robots are subjected to. The performance of three different constraint formulations are compared for a worst-case collision avoidance scenario for two robots. Preliminary numerical results demonstrate good computation simplicity/efficiency and robustness to unknown external disturbances.",[181],{"name":182,"affiliationIndices":183},"Boyang Zhang",[],[],"https://drive.google.com/file/d/1AmHj8tPZY1xf7-UumiK5ZJZyVkNN5_Mu/view",[187,188,189,190,191],"multi-robot-systems","collision-control","autonomous-robots","robot-coordination","motion-planning",["Date","2024-11-07T16:00:00.000Z"],"src/content/post/collision-control-multiple-autonomous-robots.md","3af33ade3b480d33",{"html":50,"metadata":196},{"headings":197,"localImagePaths":198,"remoteImagePaths":199,"frontmatter":200,"imagePaths":206},[],[],[],{"title":178,"abstract":179,"authors":201,"affiliations":204,"tags":205,"publishDate":63,"draft":29,"pdfUrl":185,"readingTime":64},[202],{"name":182,"affiliationIndices":203},[],[],[187,188,189,190,191],[],"conceptual-decomposition-sally-anne",{"id":207,"data":209,"filePath":229,"digest":230,"rendered":231},{"title":210,"abstract":211,"authors":212,"affiliations":222,"pdfUrl":226,"draft":29,"tags":227,"publishDate":228},"A Conceptual Primitive Decomposition of the Sally-Anne Test","Although large language models (LLMs) have been observed to perform at a human level in theory of mind tasks, deeper examinations and systematic testing of their performance in these domains is needed. Primitive decomposition representations show promise for building robotic systems with greater abilities for in-depth natural language understanding and generation. In this work, we explore representations of theory of mind which are combinations of conceptual primitives, focusing on simulations of a Sally-Anne false-belief test. We demonstrate how primitive decompositions into the conceptual building blocks of image schemas and Conceptual Dependency can represent the attribution of false beliefs to intelligent agents. The exploration has consequences for generating controlled and linguistically varied tests posed in natural language as challenge problems for large language models and for cognitive representations more broadly.",[213,216,219],{"name":214,"affiliationIndices":215},"Jamie C. Macbeth",[20],{"name":217,"affiliationIndices":218},"Boming Zhang",[24],{"name":220,"affiliationIndices":221},"Sharmin Badhan",[81],[223,224,225],"Smith College","University of Massachusetts","Independent Researcher","https://drive.google.com/file/d/1UgnW9CyAIhQV3nS4PduWMvATD6jK2I9K/view",[93,94,95],["Date","2025-11-06T11:00:00.000Z"],"src/content/post/conceptual-decomposition-sally-anne.md","6f88395d114a3ff0",{"html":50,"metadata":232},{"headings":233,"localImagePaths":234,"remoteImagePaths":235,"frontmatter":236,"imagePaths":247},[],[],[],{"title":210,"abstract":211,"authors":237,"affiliations":244,"tags":245,"publishDate":246,"draft":29,"pdfUrl":226,"readingTime":64},[238,240,242],{"name":214,"affiliationIndices":239},[20],{"name":217,"affiliationIndices":241},[24],{"name":220,"affiliationIndices":243},[81],[223,224,225],[93,94,95],"2025-11-06T11:00:00Z",[],"considerations-end-user-development-caregiving-domain",{"id":248,"data":250,"filePath":281,"digest":282,"rendered":283},{"title":251,"abstract":252,"authors":253,"affiliations":266,"pdfUrl":269,"draft":29,"tags":270,"publishDate":280},"Considerations for End-User Development in the Caregiving Domain","As service robots become more capable of autonomous behaviors, it becomes increasingly important to consider how people communicate with a robot what task it should perform and how to do the task. Accordingly, there has been a rise in attention to end-user development (EUD) interfaces, which enable non-roboticist end users to specify tasks for autonomous robots to perform. However, state-of-the-art EUD interfaces are often constrained through simplified domains or restrictive end-user interaction. Motivated by our previous qualitative design work that explored how to integrate a care robot in an assisted living community, we discuss the challenges of EUD in this complex domain. One set of challenges stems from different user-facing representations, e.g., certain tasks may lend themselves better to rule-based trigger-action representations, whereas other tasks may be easier to specify via sequences of actions. The other stems from considering the needs of multiple stakeholders, e.g., caregivers and residents of the facility may all create tasks for the robot, but the robot may not be able to share information about all tasks with all residents due to privacy concerns. We present scenarios that illustrate these challenges and also discuss possible solutions.",[254,257,260,263],{"name":255,"affiliationIndices":256},"Laura Stegner",[20],{"name":258,"affiliationIndices":259},"David Porfirio",[24],{"name":261,"affiliationIndices":262},"Mark Roberts",[24],{"name":264,"affiliationIndices":265},"Laura Hiatt",[24],[267,268],"University of Wisconsin–Madison","U.S. Naval Research Laboratory","https://drive.google.com/file/d/1CTx3HAOKUHn3wIZh21rVkoFrAfcxUPCP/view",[271,272,273,274,275,276,277,278,279],"end-user-development","caregiving","service-robots","assisted-living","human-robot-interaction","privacy","autonomous-systems","multi-stakeholder","trigger-action-programming",["Date","2023-10-25T16:00:00.000Z"],"src/content/post/considerations-end-user-development-caregiving-domain.md","aa0ca8bec4792f72",{"html":50,"metadata":284},{"headings":285,"localImagePaths":286,"remoteImagePaths":287,"frontmatter":288,"imagePaths":301},[],[],[],{"title":251,"abstract":252,"authors":289,"affiliations":298,"tags":299,"publishDate":300,"draft":29,"pdfUrl":269,"readingTime":64},[290,292,294,296],{"name":255,"affiliationIndices":291},[20],{"name":258,"affiliationIndices":293},[24],{"name":261,"affiliationIndices":295},[24],{"name":264,"affiliationIndices":297},[24],[267,268],[271,272,273,274,275,276,277,278,279],"2023-10-25T16:00:00Z",[],"embodiment-learning-representations-space-topology",{"id":302,"data":304,"filePath":327,"digest":328,"rendered":329},{"title":305,"abstract":306,"authors":307,"affiliations":314,"pdfUrl":317,"draft":29,"tags":318,"publishDate":326},"The Role of Embodiment in Learning Representations: Discovering Space Topology via Sensory-Motor Prediction","This paper explores the crucial role of embodiment in learning representations for space topology in robotics. Embodiment, the ability of an agent to interact with its environment and receive sensory feedback, is fundamental to developing accurate and efficient representations. In this work, we investigate this by applying an action-conditional prediction algorithm to data collected from a simulated environment, aiming to learn the topology of the environment through sequences of random interactions. Using a simple mobile-robot-like scenario, by leveraging sensory-motor interactions we demonstrate how the agent can discover the topology of its environment. Our results demonstrate the importance of embodiment in the development of representations and potential applicability in robotic tasks, and a simple but effective method of integrating actions into a learning loop. We suggest that building abstract representations through the use of action-conditional prediction is a step towards unification of the representations used in robotics.",[308,311],{"name":309,"affiliationIndices":310},"Oksana Hagen",[20],{"name":312,"affiliationIndices":313},"Swen Gaudl",[24],[315,316],"University of Plymouth","University of Gothenburg","https://drive.google.com/file/d/1DBDtMxey4msuWHyeczjPcWCrgApV91wp/view",[319,320,321,322,323,45,324,325],"embodiment","representation-learning","space-topology","sensory-motor-prediction","spatial-cognition","predictive-models","navigation",["Date","2024-11-08T11:00:00.000Z"],"src/content/post/embodiment-learning-representations-space-topology.md","a6c21f270a2887c6",{"html":50,"metadata":330},{"headings":331,"localImagePaths":332,"remoteImagePaths":333,"frontmatter":334,"imagePaths":343},[],[],[],{"title":305,"abstract":306,"authors":335,"affiliations":340,"tags":341,"publishDate":342,"draft":29,"pdfUrl":317,"readingTime":64},[336,338],{"name":309,"affiliationIndices":337},[20],{"name":312,"affiliationIndices":339},[24],[315,316],[319,320,321,322,323,45,324,325],"2024-11-08T11:00:00Z",[],"developing-robot-grasping-guidelines",{"id":344,"data":346,"filePath":372,"digest":373,"rendered":374},{"title":347,"abstract":348,"authors":349,"affiliations":362,"pdfUrl":364,"award":365,"draft":29,"tags":366,"publishDate":371},"Towards Developing Standards and Guidelines for Robot Grasping and Manipulation Pipelines in the COMPARE Ecosystem","The COMPARE Ecosystem aims to improve the compatibility and benchmarking of open-source products for robot manipulation through a series of activities. One such activity is the development of standards and guidelines to specify modularization practices at the component-level for individual modules (e.g., perception, grasp planning, motion planning) and integrations of components that form robot manipulation capabilities at the pipeline-level. This paper briefly reviews our work-in-progress to date to (1) build repositories of open-source products to identify common characteristics ofeach component in the pipeline, (2) investigate existing modular pipelines to glean best practices, and (3) develop new modular pipelines that advance prior work while abiding bythe proposed standards and guidelines.",[350,353,356,359],{"name":351,"affiliationIndices":352},"Huajing Zhao",[20],{"name":354,"affiliationIndices":355},"Brian Flynn",[20],{"name":357,"affiliationIndices":358},"Adam Norton",[20],{"name":360,"affiliationIndices":361},"Holly Yanco",[20],[363],"University of Massachusetts Lowell","https://drive.google.com/file/d/1I4Rif_rikqw6ANX-XRtvyLUwk0TDxpas/view","Best Paper Award",[367,368,369,370],"compare","guidelines","grasping","manipulation",["Date","2025-11-06T11:00:00.000Z"],"src/content/post/developing-robot-grasping-guidelines.md","2ad1880a147e7527",{"html":50,"metadata":375},{"headings":376,"localImagePaths":377,"remoteImagePaths":378,"frontmatter":379,"imagePaths":392},[],[],[],{"title":347,"abstract":348,"authors":380,"affiliations":389,"tags":390,"publishDate":246,"draft":29,"pdfUrl":364,"award":365,"readingTime":64},[381,383,385,387],{"name":351,"affiliationIndices":382},[20],{"name":354,"affiliationIndices":384},[20],{"name":357,"affiliationIndices":386},[20],{"name":360,"affiliationIndices":388},[20],[363],[391,368,369,370],"COMPARE",[],"embracing-lag-real-time-challenges-multi-agent-systems",{"id":393,"data":395,"filePath":423,"digest":424,"rendered":425},{"title":396,"abstract":397,"authors":398,"affiliations":408,"pdfUrl":411,"draft":29,"tags":412,"publishDate":422},"Embracing the Lag: Real-Time Challenges in Multi-Agent Systems","In the dynamic world of distributed multi-agent systems, where agents collaborate, coordinate, and make decisions in real time, the concept of 'lag' poses a formidable challenge. As these autonomous entities interact across vast networks, communication delays and latency inevitably arise, impacting the system's correctness. It is this inherent problem against the ticking clock that drives our research. Consider the arenas of distributed video games, self-driving vehicles, and online trading and auction systems. Each of these is impacted by latency. While there are several methods employed to combat the effects of latency in these arenas, it is notable that latency itself is not explicitly accounted for, particularly in POMDP models. We argue that latency needs to be modeled.",[399,402,405],{"name":400,"affiliationIndices":401},"Sarah Dumnich",[20],{"name":403,"affiliationIndices":404},"William Birmingham",[20],{"name":406,"affiliationIndices":407},"Britton Wolfe",[24],[409,410],"Saint Vincent College","Grove City College","https://drive.google.com/file/d/1MZVsWrepiq9b8RScGHK-6y3UycSnx5Jy/view",[413,414,415,416,417,418,277,419,420,421],"multi-agent-systems","real-time-systems","latency","distributed-systems","communication-delays","pomdp","coordination","video-games","self-driving-vehicles",["Date","2023-10-25T11:00:00.000Z"],"src/content/post/embracing-lag-real-time-challenges-multi-agent-systems.md","c29033a1faabd94e",{"html":50,"metadata":426},{"headings":427,"localImagePaths":428,"remoteImagePaths":429,"frontmatter":430,"imagePaths":441},[],[],[],{"title":396,"abstract":397,"authors":431,"affiliations":438,"tags":439,"publishDate":440,"draft":29,"pdfUrl":411,"readingTime":64},[432,434,436],{"name":400,"affiliationIndices":433},[20],{"name":403,"affiliationIndices":435},[20],{"name":406,"affiliationIndices":437},[24],[409,410],[413,414,415,416,417,418,277,419,420,421],"2023-10-25T11:00:00Z",[],"forgetful-large-language-models-robot-programming",{"id":442,"data":444,"filePath":466,"digest":467,"rendered":468},{"title":445,"abstract":446,"authors":447,"affiliations":454,"pdfUrl":456,"draft":29,"tags":457,"publishDate":465},"Forgetful Large Language Models: Lessons Learned from Using LLMs in Robot Programming","Large language models offer new ways of empowering people to program robot applications—namely, code generation via prompting. However, the code generated by LLMs is susceptible to errors. This work reports a preliminary exploration that empirically characterizes common errors produced by LLMs in robot programming. We categorize these errors into two phases: interpretation and execution. In this work, we focus on errors in execution and observe that they are caused by LLMs being “forgetful” of key information provided in user prompts. Based on this observation, we propose prompt engineering tactics designed to reduce errors in execution. We then demonstrate the effectiveness of these tactics with three language models: ChatGPT, Bard, and LLaMA-2. Finally, we discuss lessons learned from using LLMs in robot programming and call for the benchmarking of LLM-powered end-user development of robot applications.",[448,451],{"name":449,"affiliationIndices":450},"Juo-Tung Chen",[20],{"name":452,"affiliationIndices":453},"Chien-Ming Huang",[20],[455],"Johns Hopkins University","https://drive.google.com/file/d/11jpPuz_d4JdqNHEFkN3z1--b3JuL-5M7/view",[458,459,460,461,462,463,271,464],"large-language-models","robot-programming","code-generation","prompt-engineering","chatgpt","machine-learning","error-analysis",["Date","2023-10-26T14:00:00.000Z"],"src/content/post/forgetful-large-language-models-robot-programming.md","05740ef5460666bc",{"html":50,"metadata":469},{"headings":470,"localImagePaths":471,"remoteImagePaths":472,"frontmatter":473,"imagePaths":482},[],[],[],{"title":445,"abstract":446,"authors":474,"affiliations":479,"tags":480,"publishDate":481,"draft":29,"pdfUrl":456,"readingTime":64},[475,477],{"name":449,"affiliationIndices":476},[20],{"name":452,"affiliationIndices":478},[20],[455],[458,459,460,461,462,463,271,464],"2023-10-26T14:00:00Z",[],"goals-vs-actions-robot-programming",{"id":483,"data":485,"filePath":501,"digest":502,"rendered":503},{"title":486,"abstract":487,"authors":488,"affiliations":492,"pdfUrl":493,"draft":29,"tags":494,"publishDate":500},"Goals vs. Actions as User-Facing Representations for Robot Programming","Robot application development (RAD) tools provide novice developers with the ability to specify what tasks a robot needs to perform and how it should perform these tasks. Existing RAD tools often focus on how end-user developer intent should be captured and stored as a computational artifact, including but not limited to mixed-reality interfaces, natural language programming, and visual programming environments. However, these tools fail to incorporate robot intelligence in the development pipeline, making it cumbersome for end-user developers to specify a detailed sequence of steps for the robot to complete a task, as well as overconstraining the robot. We thereby suggest that robot intelligence needs to be incorporated into RAD tools. Our proposed work will focus on balancing the amount and nature of domain knowledge that the end-user developer must provide to the robot with the robot’s ability to plan and act autonomously, as well as how the end-user’s domain knowledge should be represented as a computational artifact. In particular, we will be comparing two different approaches – goals versus actions – as a user-facing representation for collecting developer task specifications and intent while leveraging the robot’s ability to plan and act autonomously through robot intelligence by extending the Polaris interface, developed by Porfirio, Roberts, and Hiatt (2024), through a user study.",[489],{"name":490,"affiliationIndices":491},"Nhi Tran",[20],[268],"https://drive.google.com/file/d/1E1xZMXu1w4NgvCXlS96RJAJaCFl7SP35/view",[459,495,496,497,275,498,499,45],"user-interfaces","goal-based-programming","action-based-programming","programming-paradigms","user-experience",["Date","2024-11-08T16:00:00.000Z"],"src/content/post/goals-vs-actions-robot-programming.md","8efee29c0dae35c2",{"html":50,"metadata":504},{"headings":505,"localImagePaths":506,"remoteImagePaths":507,"frontmatter":508,"imagePaths":515},[],[],[],{"title":486,"abstract":487,"authors":509,"affiliations":512,"tags":513,"publishDate":514,"draft":29,"pdfUrl":493,"readingTime":64},[510],{"name":490,"affiliationIndices":511},[20],[268],[459,495,496,497,275,498,499,45],"2024-11-08T16:00:00Z",[],"ground-manipulator-primitive-tasks-executable-actions-llms",{"id":516,"data":518,"filePath":541,"digest":542,"rendered":543},{"title":519,"abstract":520,"authors":521,"affiliations":528,"pdfUrl":530,"draft":29,"tags":531,"publishDate":540},"Ground Manipulator Primitive Tasks to Executable Actions using Large Language Models","Layered architectures have been widely used in robot systems. The majority of them implement planning and execution functions in separate layers. However, there still lacks a straightforward way to transit high-level tasks in the planning layer to the low-level motor commands in the execution layer. In order to tackle this challenge, we propose a novel approach to ground the manipulator primitive tasks to robot low-level actions using large language models (LLMs). We designed a program-function-like prompt based on the task frame formalism. In this way, we enable LLMs to generate position/force set-points for hybrid control. Evaluations over several state-of-the-art LLMs are provided.",[522,525],{"name":523,"affiliationIndices":524},"Yue Cao",[20],{"name":526,"affiliationIndices":527},"C. S. George Lee",[20],[529],"Purdue University","https://drive.google.com/file/d/1OfmcGT-E-9faA3_v5CfZ9Rl4Ellw5xbR/view",[532,533,534,535,536,537,538,539],"large language models","robot manipulation","task grounding","hybrid control","robot planning","primitive tasks","execution","llms",["Date","2023-10-26T11:00:00.000Z"],"src/content/post/ground-manipulator-primitive-tasks-executable-actions-llms.md","9d6f97e1239c3acc",{"html":50,"metadata":544},{"headings":545,"localImagePaths":546,"remoteImagePaths":547,"frontmatter":548,"imagePaths":558},[],[],[],{"title":519,"abstract":520,"authors":549,"affiliations":554,"tags":555,"publishDate":557,"draft":29,"pdfUrl":530,"readingTime":64},[550,552],{"name":523,"affiliationIndices":551},[20],{"name":526,"affiliationIndices":553},[20],[529],[532,533,534,535,536,537,538,556],"LLMs","2023-10-26T11:00:00Z",[],"grounding-complex-natural-language-commands-temporal-tasks",{"id":559,"data":561,"filePath":598,"digest":599,"rendered":600},{"title":562,"abstract":563,"authors":564,"affiliations":586,"pdfUrl":589,"draft":29,"tags":590,"publishDate":597},"Grounding Complex Natural Language Commands for Temporal Tasks in Unseen Environments","Grounding navigational commands to linear temporal logic (LTL) leverages its unambiguous semantics for reasoning about long-horizon tasks and verifying the satisfaction of temporal constraints. Existing approaches require training data from the specific environment and landmarks that will be used in natural language to understand commands in those environments. We propose Lang2LTL, a modular system and a software package that leverages large language models (LLMs) to ground temporal navigational commands to LTL specifications in environments without prior language data. We comprehensively evaluate Lang2LTL for five well-defined generalization behaviors. Lang2LTL demonstrates the state-of-the-art ability of a single model to ground navigational commands to diverse temporal specifications in 21 city-scaled environments. Finally, we demonstrate a physical robot using Lang2LTL can follow 52 semantically diverse navigational commands in two indoor environments.",[565,568,571,574,577,580,583],{"name":566,"affiliationIndices":567},"Jason Liu",[20],{"name":569,"affiliationIndices":570},"Ziyi Yang",[20],{"name":572,"affiliationIndices":573},"Ifrah Idrees",[20],{"name":575,"affiliationIndices":576},"Sam Liang",[24],{"name":578,"affiliationIndices":579},"Benjamin Schornstein",[20],{"name":581,"affiliationIndices":582},"Stefanie Tellex",[20],{"name":584,"affiliationIndices":585},"Ankit Shah",[20],[587,588],"Brown University","Princeton University","https://drive.google.com/file/d/1wHmKXkOwN8PWgxfXJNed7uDL2b69XOFR/view",[591,592,593,532,325,594,536,595,539,596],"natural language processing","linear temporal logic","ltl","temporal tasks","language grounding","long-horizon tasks",["Date","2023-10-25T16:00:00.000Z"],"src/content/post/grounding-complex-natural-language-commands-temporal-tasks.md","def9860806aca6e8",{"html":50,"metadata":601},{"headings":602,"localImagePaths":603,"remoteImagePaths":604,"frontmatter":605,"imagePaths":624},[],[],[],{"title":562,"abstract":563,"authors":606,"affiliations":621,"tags":622,"publishDate":300,"draft":29,"pdfUrl":589,"readingTime":64},[607,609,611,613,615,617,619],{"name":566,"affiliationIndices":608},[20],{"name":569,"affiliationIndices":610},[20],{"name":572,"affiliationIndices":612},[20],{"name":575,"affiliationIndices":614},[24],{"name":578,"affiliationIndices":616},[20],{"name":581,"affiliationIndices":618},[20],{"name":584,"affiliationIndices":620},[20],[587,588],[591,592,623,532,325,594,536,595,556,596],"LTL",[],"interactive-digital-testimonies-framework",{"id":625,"data":627,"filePath":652,"digest":653,"rendered":654},{"title":628,"abstract":629,"authors":630,"affiliations":640,"pdfUrl":642,"draft":29,"tags":643,"publishDate":651},"Promoting transparent and consistent frameworks for interactive digital testimonies: A case study on preserving Zilli Schmidt's story","Interactive Digital Testimonies (IDTs) combine digital archives of purpose-made recordings, conversational agents, and immersive display technology to preserve and recreate interactive conversations with contemporary witnesses in a lifelike manner. IDTs represent a specific subcategory of (Embodied) Conversational Agents (ECAs) due to the constraint of not including AI-generated or otherwise synthetic responses or actions. While numerous IDTs have been developed over the last few years, the descriptions of these systems and their respective evaluations frequently lack consistency and transparency, which has led to considerable heterogeneity and a lack of comparability. To counteract these developments, we present the IDT of Holocaust survivor and member of the German-speaking Romani community Zilli Schmidt, which we developed since 2021. We transparently share both content-related and technical features of this IDT.",[631,634,637],{"name":632,"affiliationIndices":633},"Fabian Heindl",[20],{"name":635,"affiliationIndices":636},"Daniel Kolb",[24],{"name":638,"affiliationIndices":639},"Markus Gloe",[20],[641,26],"Ludwig-Maximilians-Universitat München","https://drive.google.com/file/d/1puzvK4TcZ_71dXil1jSFpggXJwaVmksJ/view",[644,645,646,647,648,649,650],"digital-testimonies","interactive-systems","historical-preservation","ethical-ai","human-computer-interaction","memorial-technology","transparency",["Date","2024-11-07T11:00:00.000Z"],"src/content/post/interactive-digital-testimonies-framework.md","4ec39ae02b5f3100",{"html":50,"metadata":655},{"headings":656,"localImagePaths":657,"remoteImagePaths":658,"frontmatter":659,"imagePaths":670},[],[],[],{"title":628,"abstract":629,"authors":660,"affiliations":667,"tags":668,"publishDate":669,"draft":29,"pdfUrl":642,"readingTime":64},[661,663,665],{"name":632,"affiliationIndices":662},[20],{"name":635,"affiliationIndices":664},[24],{"name":638,"affiliationIndices":666},[20],[641,26],[644,645,646,647,648,649,650],"2024-11-07T11:00:00Z",[],"investigating-disclosure-personal-information-robots",{"id":671,"data":673,"filePath":695,"digest":696,"rendered":697},{"title":674,"abstract":675,"authors":676,"affiliations":680,"pdfUrl":682,"draft":29,"tags":683,"publishDate":694},"Investigating Disclosure of Personal Information to Robots as a Function of Robot Appearance and Level of Self-Disclosure","Human-robot interaction consists of a rich set of behaviors between humans and robots often requiring the exchange of personal and sensitive information between them. From a conceptual framework this paper discusses whether a robot who self-discloses personal information when conversing with a user will prompt the user to reciprocate and self-disclose personal and sensitive information to the robot. Additionally, the paper discusses various factors which may influence whether self-disclosure of personal information between human and robot occurs and briefly discusses aspects of a conceptual representational system necessary for HRI enabling the robot to self-disclose to a user.",[677],{"name":678,"affiliationIndices":679},"Jessica Barfield",[20],[681],"University of Tennessee- Knoxville","https://drive.google.com/file/d/1Ad6FRO6TL-Bj_UwYmHB5MIEO5r85WUBE/view",[684,685,686,687,688,276,689,690,691,692,693],"human-robot interaction","hri","self-disclosure","personal information","robot appearance","trust","conversation","social robotics","information sharing","behavioral interaction",["Date","2023-10-25T16:00:00.000Z"],"src/content/post/investigating-disclosure-personal-information-robots.md","3919deb3c2e32ff2",{"html":50,"metadata":698},{"headings":699,"localImagePaths":700,"remoteImagePaths":701,"frontmatter":702,"imagePaths":709},[],[],[],{"title":674,"abstract":675,"authors":703,"affiliations":706,"tags":707,"publishDate":300,"draft":29,"pdfUrl":682,"readingTime":64},[704],{"name":678,"affiliationIndices":705},[20],[681],[684,708,686,687,688,276,689,690,691,692,693],"HRI",[],"interface-design-learning-demonstration-older-adults",{"id":710,"data":712,"filePath":741,"digest":742,"rendered":743},{"title":713,"abstract":714,"authors":715,"affiliations":728,"pdfUrl":729,"draft":29,"tags":730,"publishDate":740},"Interface Design for Learning from Demonstration with Older Adults","Assistive in-home robots have the potential to enable older adults to age in place, by allowing older adults to offload mentally and physically demanding tasks to a robot. However, one challenge for in-home robots is that each individual will have differing needs, preferences, and home environments, which all change over time. Learning from Demonstration (LfD) is one solution to enable non-expert users to communicate their differing and changing needs and preferences to a robot, but LfD is underexplored with a population of older adults. We aim to develop a user interface that is intuitive for older adults and facilitates their ability to teach a robot via LfD. In this paper, we conduct a human-subjects experiment to evaluate the usability of a robot manipulation and recording interface with a target population of older adults. Additionally, we propose a full study with a robot to understand the feasibility of LfD for older adults.",[716,719,722,725],{"name":717,"affiliationIndices":718},"Lakshmi Seelam",[20],{"name":720,"affiliationIndices":721},"Erin Hedlund-Botti",[20],{"name":723,"affiliationIndices":724},"Chuxuan Yang",[20],{"name":726,"affiliationIndices":727},"Matthew Gombolay",[20],[89],"https://drive.google.com/file/d/1H2ZkY_59Ww4H3WjYfPtPqsnnIaSnIw9z/view",[731,732,733,734,735,684,736,737,738,739,533],"interface design","learning from demonstration","lfd","older adults","assistive robots","user interface","usability","aging in place","human subjects study",["Date","2023-10-25T11:00:00.000Z"],"src/content/post/interface-design-learning-demonstration-older-adults.md","3fdc682715c35561",{"html":50,"metadata":744},{"headings":745,"localImagePaths":746,"remoteImagePaths":747,"frontmatter":748,"imagePaths":761},[],[],[],{"title":713,"abstract":714,"authors":749,"affiliations":758,"tags":759,"publishDate":440,"draft":29,"pdfUrl":729,"readingTime":64},[750,752,754,756],{"name":717,"affiliationIndices":751},[20],{"name":720,"affiliationIndices":753},[20],{"name":723,"affiliationIndices":755},[20],{"name":726,"affiliationIndices":757},[20],[89],[731,732,760,734,735,684,736,737,738,739,533],"LfD",[],"lang2ltl-2-spatiotemporal-navigation-commands",{"id":762,"data":764,"filePath":792,"digest":793,"rendered":794},{"title":765,"abstract":766,"authors":767,"affiliations":781,"pdfUrl":782,"draft":29,"tags":783,"publishDate":791},"Lang2LTL-2: Grounding Spatiotemporal Navigation Commands Using Large Language and Vision-Language Models","Grounding spatiotemporal navigation commands to structured task specifications enables autonomous robots to understand a broad range of natural language and solve long-horizon tasks with safety guarantees. Prior works mostly focus on grounding spatial or temporally extended language for robots. We propose Lang2LTL-2, a modular system that leverages pretrained large language and vision-language models and multimodal semantic information to ground spatiotemporal navigation commands in novel city-scaled environments without retraining. Lang2LTL-2 achieves 93.53% language grounding accuracy on a dataset of 21,780 semantically diverse natural language commands in unseen environments. We run an ablation study to validate the need for different modalities. We also show that a physical robot equipped with the same system without modification can execute 50 semantically diverse natural language commands in both indoor and outdoor environments.",[768,771,773,776,778],{"name":769,"affiliationIndices":770},"Jason Xinyu Liu",[20],{"name":584,"affiliationIndices":772},[20],{"name":774,"affiliationIndices":775},"George Konidaris",[20],{"name":581,"affiliationIndices":777},[20],{"name":779,"affiliationIndices":780},"David Paulius",[20],[587],"https://drive.google.com/file/d/1XCKm9-Sip3zqcpGEtvtaNLrpO0UdMWna/view",[784,785,33,786,458,787,788,789,790],"natural-language-processing","linear-temporal-logic","language-grounding","vision-language-models","spatiotemporal-reasoning","formal-methods","navigation-commands",["Date","2024-11-08T16:00:00.000Z"],"src/content/post/lang2ltl-2-spatiotemporal-navigation-commands.md","7f2452fb1d177a15",{"html":50,"metadata":795},{"headings":796,"localImagePaths":797,"remoteImagePaths":798,"frontmatter":799,"imagePaths":813},[],[],[],{"title":765,"abstract":766,"authors":800,"affiliations":811,"tags":812,"publishDate":514,"draft":29,"pdfUrl":782,"readingTime":64},[801,803,805,807,809],{"name":769,"affiliationIndices":802},[20],{"name":584,"affiliationIndices":804},[20],{"name":774,"affiliationIndices":806},[20],{"name":581,"affiliationIndices":808},[20],{"name":779,"affiliationIndices":810},[20],[587],[784,785,33,786,458,787,788,789,790],[],"human-perception-robot-failure-explanation",{"id":814,"data":816,"filePath":837,"digest":838,"rendered":839},{"title":817,"abstract":818,"authors":819,"affiliations":829,"pdfUrl":831,"draft":29,"tags":832,"publishDate":836},"Human Perception of Robot Failure Explanation During a Pick and Place Task","In recent years, researchers have extensively used non-verbal gestures, such as head and arm movements, to express a robot’s intentions and capabilities to humans. Inspired by past research, we investigated how different explanation modalities can aid human understanding and perception of how robots communicate failures and provide explanations during block pick-and-place tasks. Through an in-person, within-subjects experiment with 24 participants, we studied four modes of explanations across four types of failures. Some of these were chosen to mimic combinations from prior work in order to both extend and replicate past findings by the community. We found that speech explanations were preferred to non-verbal and visual cues in terms of similarity to humans. Additionally, projected images had a comparable effect on explanation as other non-verbal modules. We also found consistent results with a prior online study.",[820,823,826],{"name":821,"affiliationIndices":822},"Huy Quyen Ngo",[20],{"name":824,"affiliationIndices":825},"Elizabeth Carter",[20],{"name":827,"affiliationIndices":828},"Aaron Steinfeld",[20],[830],"The Robotics Institute, Carnegie Mellon University","https://drive.google.com/file/d/18C_93JffqNQfJKy78PF-Iq5ZgwlIISOi/view",[275,833,834,835,45,689,370],"failure-explanation","perception","pick-and-place",["Date","2024-11-07T11:00:00.000Z"],"src/content/post/human-perception-robot-failure-explanation.md","fee80f6bd30b6734",{"html":50,"metadata":840},{"headings":841,"localImagePaths":842,"remoteImagePaths":843,"frontmatter":844,"imagePaths":854},[],[],[],{"title":817,"abstract":818,"authors":845,"affiliations":852,"tags":853,"publishDate":669,"draft":29,"pdfUrl":831,"readingTime":64},[846,848,850],{"name":821,"affiliationIndices":847},[20],{"name":824,"affiliationIndices":849},[20],{"name":827,"affiliationIndices":851},[20],[830],[275,833,834,835,45,689,370],[],"language-gesture-virtual-reality",{"id":855,"data":857,"filePath":882,"digest":883,"rendered":884},{"title":858,"abstract":859,"authors":860,"affiliations":873,"pdfUrl":876,"draft":29,"tags":877,"publishDate":881},"Language and Gesture in Virtual Reality: Is a Gesture Worth 1000 Words?","Robots are increasingly incorporating multimodal information and human signals to resolve ambiguity in embodied human-robot interaction. Harnessing signals such as gestures may expedite robot exploration in large, outdoor urban environments for supporting disaster recovery operations, where speech may be unclear due to noise or the challenges of a dynamic and dangerous environment. Despite this potential, capturing human gesture and properly grounding it to crowded, outdoor environments remains a challenge. In this work, we propose a method to model human gesture and ground it to spoken language instructions given to a robot for execution in large spaces. We implement our method in virtual reality to develop a workflow for faster future data collection. We present a series of proposed experiments that compare a language-only baseline to our proposed language supplemented by gesture approach, and discuss how our approach has the potential to reinforce the human’s intent and detect discrepancies in gesture and spoken instructions in these large and crowded environments.",[861,864,867,870],{"name":862,"affiliationIndices":863},"Padraig\tHiggins",[20],{"name":865,"affiliationIndices":866},"Cory\tHayes",[24],{"name":868,"affiliationIndices":869},"Stephanie\tLukin",[24],{"name":871,"affiliationIndices":872},"Cynthia\tMatuszek",[20],[874,875],"University of Maryland","Army Research Lab","https://drive.google.com/file/d/1GEvchCIvUcA2Chep6IcTJ9Qtk3mOI8Uk/view",[878,879,880],"gesture","virtual reality","language",["Date","2025-11-07T11:00:00.000Z"],"src/content/post/language-gesture-virtual-reality.md","4543a64fe97a80f4",{"html":50,"metadata":885},{"headings":886,"localImagePaths":887,"remoteImagePaths":888,"frontmatter":889,"imagePaths":901},[],[],[],{"title":858,"abstract":859,"authors":890,"affiliations":899,"tags":900,"publishDate":116,"draft":29,"pdfUrl":876,"readingTime":64},[891,893,895,897],{"name":862,"affiliationIndices":892},[20],{"name":865,"affiliationIndices":894},[24],{"name":868,"affiliationIndices":896},[24],{"name":871,"affiliationIndices":898},[20],[874,875],[878,879,880],[],"homerobot-open-source-software-stack-mobile-manipulation",{"id":902,"data":904,"filePath":955,"digest":956,"rendered":957},{"title":905,"abstract":906,"authors":907,"affiliations":936,"pdfUrl":942,"draft":29,"tags":943,"publishDate":954},"HomeRobot: An Open Source Software Stack for Mobile Manipulation Research","Reproducibility in robotics research requires capable, shared hardware platforms which can be used for a wide variety of research. We have seen the power of these sorts of shared platforms in more general machine learning research (e.g., PyTorch), where there is a constant and open-sourced development over time to meet the needs of the community To be able to make rapid progress in robotics in the same way, we propose that we need: (1) shared real-world platforms which allow different teams to test and compare methods at low cost; (2) challenging simulations that reflect real-world environments and especially can drive perception and planning research; and (3) low-cost platforms with enough software to get started addressing all of these problems. To this end, we propose HomeRobot, a mobile manipulator software stack with associated benchmark in simulation, which is initially based on the low-cost, human-safe Hello Robot Stretch.",[908,911,914,917,920,923,926,930,933],{"name":909,"affiliationIndices":910},"Chris Paxton",[20],{"name":912,"affiliationIndices":913},"Austin Wang",[20],{"name":915,"affiliationIndices":916},"Binit Shah",[24],{"name":918,"affiliationIndices":919},"Blaine Matulevich",[24],{"name":921,"affiliationIndices":922},"Dhruv Shah",[20],{"name":924,"affiliationIndices":925},"Karmesh Yadav",[20,81],{"name":927,"affiliationIndices":928},"Sriram Ramakrishnan",[929],4,{"name":931,"affiliationIndices":932},"Sriram Yenamandra",[81],{"name":934,"affiliationIndices":935},"Yonatan Bisk",[20,85],[937,938,939,940,941],"FAIR, Meta AI","Hello Robot","Georgia Tech","Carnegie Mellon","UT Austin","https://drive.google.com/file/d/1-_fNMoTJ9pwEVP9RqqV53oZrLmcGdAYS/view",[944,945,946,947,948,949,950,951,834,952,953],"open source","software stack","mobile manipulation","robotics research","simulation","hardware platforms","hello robot stretch","benchmarks","planning","reproducibility",["Date","2023-10-26T14:00:00.000Z"],"src/content/post/homerobot-open-source-software-stack-mobile-manipulation.md","a4e6163d1dcbbbfb",{"html":50,"metadata":958},{"headings":959,"localImagePaths":960,"remoteImagePaths":961,"frontmatter":962,"imagePaths":985},[],[],[],{"title":905,"abstract":906,"authors":963,"affiliations":982,"tags":983,"publishDate":481,"draft":29,"pdfUrl":942,"readingTime":64},[964,966,968,970,972,974,976,978,980],{"name":909,"affiliationIndices":965},[20],{"name":912,"affiliationIndices":967},[20],{"name":915,"affiliationIndices":969},[24],{"name":918,"affiliationIndices":971},[24],{"name":921,"affiliationIndices":973},[20],{"name":924,"affiliationIndices":975},[20,81],{"name":927,"affiliationIndices":977},[929],{"name":931,"affiliationIndices":979},[81],{"name":934,"affiliationIndices":981},[20,85],[937,938,939,940,941],[944,945,946,947,948,949,984,951,834,952,953],"Hello Robot Stretch",[],"large-scale-knowledge-graphs-robotic-perception",{"id":986,"data":988,"filePath":1014,"digest":1015,"rendered":1016},{"title":989,"abstract":990,"authors":991,"affiliations":1004,"pdfUrl":1006,"draft":29,"tags":1007,"publishDate":1013},"Large Scale Knowledge Graphs as a Tool for Enhanced Robotic Perception","Autonomous robotic systems depend on their perception and understanding of their environment for informed decision-making. One of the goals of the Semantic Web is to make knowledge on the Web machine-readable, which can significantly aid robots by providing background knowledge, and thereby support their understanding. In this paper, we present a reasoning system that uses the Ontology for Robotic Knowledge Acquisition (ORKA) to integrate the sensory data and perception algorithms of the robot, thereby enhancing its autonomous capabilities. This reasoning system is subsequently employed to retrieve and integrate information from the Semantic Web, thereby improving the robot’s comprehension of its environment. To achieve this, the system employs a Perceived-Entity Linking (PEL) pipeline that associates regions in the sensory data of the robotic agent with concepts in a target knowledge graph. As a use-case for the linking process, the Perceived-Entity Typing task is used to determine the more fine-grained subclass of the perceived entities. Specifically, we provide an analysis of the performance of different knowledge graph embedding methods on the task using a annotated observations and WikiData as a target knowledge graph. The experiments indicate that relying on pre-trained embedding methods results in an increased performance when using TransE as the embedding method for the observations of the robot. This contribution advances the field by demonstrating the potential of integrating Semantic Web technologies with robotic perception, thereby enabling more nuanced and context-aware decision-making in autonomous systems.",[992,995,998,1001],{"name":993,"affiliationIndices":994},"Mark Adamik",[20],{"name":996,"affiliationIndices":997},"Ilaria Tiddi",[20],{"name":999,"affiliationIndices":1000},"Romana Pernisch",[20],{"name":1002,"affiliationIndices":1003},"Stefan Schlobach",[20],[1005],"Vrije Universiteit Amsterdam","https://drive.google.com/file/d/1BbQU7IvI4GV9NOE0PY5XieekeqyTvKm3/view",[1008,1009,1010,37,148,1011,1012],"knowledge-graphs","robotic-perception","semantic-understanding","scene-understanding","cognitive-robotics",["Date","2024-11-07T16:00:00.000Z"],"src/content/post/large-scale-knowledge-graphs-robotic-perception.md","990c6cf83bf4c542",{"html":50,"metadata":1017},{"headings":1018,"localImagePaths":1019,"remoteImagePaths":1020,"frontmatter":1021,"imagePaths":1033},[],[],[],{"title":989,"abstract":990,"authors":1022,"affiliations":1031,"tags":1032,"publishDate":63,"draft":29,"pdfUrl":1006,"readingTime":64},[1023,1025,1027,1029],{"name":993,"affiliationIndices":1024},[20],{"name":996,"affiliationIndices":1026},[20],{"name":999,"affiliationIndices":1028},[20],{"name":1002,"affiliationIndices":1030},[20],[1005],[1008,1009,1010,37,148,1011,1012],[],"latent-hypothesis-exploration",{"id":1034,"data":1036,"filePath":1050,"digest":1051,"rendered":1052},{"title":1037,"abstract":211,"authors":1038,"affiliations":1045,"pdfUrl":1046,"draft":29,"tags":1047,"publishDate":1049},"Latent Hypothesis-Planned Exploration",[1039,1042],{"name":1040,"affiliationIndices":1041},"Maxwell J. Jacobson",[20],{"name":1043,"affiliationIndices":1044},"Yexiang Xue",[20],[529],"https://drive.google.com/file/d/1eRcaX_kvUHi_RrgYg3nHGAJitugMLoUV/view",[539,94,1048,532],"concepts",["Date","2025-11-06T11:00:00.000Z"],"src/content/post/latent-hypothesis-exploration.md","df9055418a71e4d1",{"html":50,"metadata":1053},{"headings":1054,"localImagePaths":1055,"remoteImagePaths":1056,"frontmatter":1057,"imagePaths":1065},[],[],[],{"title":1037,"abstract":211,"authors":1058,"affiliations":1063,"tags":1064,"publishDate":246,"draft":29,"pdfUrl":1046,"readingTime":64},[1059,1061],{"name":1040,"affiliationIndices":1060},[20],{"name":1043,"affiliationIndices":1062},[20],[529],[539,94,1048,532],[],"learning-multi-modal-whole-body-control",{"id":1066,"data":1068,"filePath":1097,"digest":1098,"rendered":1099},{"title":1069,"abstract":1070,"authors":1071,"affiliations":1087,"pdfUrl":1089,"award":1090,"draft":29,"tags":1091,"publishDate":1096},"Learning Multi-Modal Whole-Body Control for Real-World Humanoid Robots","A major challenge in humanoid robotics is designing a unified interface for commanding diverse whole-body behaviors, from precise footstep sequences to partial-body mimicry and joystick teleoperation. We introduce the Masked Humanoid Controller (MHC), a learned whole-body controller that exposes a simple yet expressive interface: the specification of masked target trajectories over selected subsets of the robot’s state variables. This unified abstraction allows high-level systems to issue commands in a flexible format that accommodates multi-modal inputs such as optimized trajectories, motion capture clips, re-targeted video, and real-time joystick signals. The MHC is trained in simulation using a curriculum that spans this full range of modalities, enabling robust execution of partially specified behaviors while maintaining balance and disturbance rejection. We demonstrate the MHC both in simulation and on the real-world Digit V3 humanoid, showing that a single learned controller is capable of executing such diverse whole-body commands in the real world through a common representational interface.",[1072,1075,1078,1081,1084],{"name":1073,"affiliationIndices":1074},"Pranay Dugar",[20],{"name":1076,"affiliationIndices":1077},"Aayam Shrestha",[20],{"name":1079,"affiliationIndices":1080},"Fangzhou Yu",[20],{"name":1082,"affiliationIndices":1083},"Bart van Marum",[20],{"name":1085,"affiliationIndices":1086},"Alan Fern",[20],[1088],"Oregon State University","https://drive.google.com/file/d/1vDoztcDXlCp0P1TAmGp-l9L9mtYQGerF/view","Best Paper Runner Up",[1092,1093,1094,1095],"humanoids","controllers","whole-body","learning",["Date","2025-11-06T11:00:00.000Z"],"src/content/post/learning-multi-modal-whole-body-control.md","815f7897fca07ea4",{"html":50,"metadata":1100},{"headings":1101,"localImagePaths":1102,"remoteImagePaths":1103,"frontmatter":1104,"imagePaths":1118},[],[],[],{"title":1069,"abstract":1070,"authors":1105,"affiliations":1116,"tags":1117,"publishDate":246,"draft":29,"pdfUrl":1089,"award":1090,"readingTime":64},[1106,1108,1110,1112,1114],{"name":1073,"affiliationIndices":1107},[20],{"name":1076,"affiliationIndices":1109},[20],{"name":1079,"affiliationIndices":1111},[20],{"name":1082,"affiliationIndices":1113},[20],{"name":1085,"affiliationIndices":1115},[20],[1088],[1092,1093,1094,1095],[],"lightweight-probabilistic-planning-macro-actions",{"id":1119,"data":1121,"filePath":1148,"digest":1149,"rendered":1150},{"title":1122,"abstract":1123,"authors":1124,"affiliations":1137,"pdfUrl":1138,"draft":29,"tags":1139,"publishDate":1147},"Lightweight Probabilistic Planning with Macro Actions","In assigning task goals to a robot, the locations of objects relevant to these goals may not be known beforehand. Off-the-shelf probabilistic planning approaches can then be used to generate a policy for achieving these goals. If these approaches are run online, the robot can continuously plan and re-plan during the execution of its task. Unfortunately, online probabilistic planning requires substantial compute resources like time and memory. In cases where the planner must run onboard a consumer robot (e.g., in a household) or in isolated environments (e.g., onboard a ship), the robot may lack sufficient compute resources for online probabilistic planning, thereby necessitating the development of more lightweight solutions. In this paper, we describe our in-progress research on PUMA, a lightweight variant of the PO-UCT probabilistic planning algorithm that modifies the PO-UCT search tree representation to include macro actions. Macro actions compress deterministic branches of the search tree, which speed up search, though at the expense of the algorithm’s ability to guarantee convergence to an optimal solution.",[1125,1128,1130,1133,1135],{"name":1126,"affiliationIndices":1127},"Evan Conway",[],{"name":258,"affiliationIndices":1129},[20],{"name":1131,"affiliationIndices":1132},"David Chan",[],{"name":261,"affiliationIndices":1134},[20],{"name":264,"affiliationIndices":1136},[20],[268],"https://drive.google.com/file/d/1-35bt5P-h9Pvh_9CzeHu-eSHzzf4WLv9/view",[1140,1141,1142,45,1143,1144,1145,1146],"probabilistic-planning","macro-actions","hierarchical-planning","computational-efficiency","uncertainty","decision-making","ai-planning",["Date","2024-11-08T11:00:00.000Z"],"src/content/post/lightweight-probabilistic-planning-macro-actions.md","2ebac4c8526ce7d8",{"html":50,"metadata":1151},{"headings":1152,"localImagePaths":1153,"remoteImagePaths":1154,"frontmatter":1155,"imagePaths":1169},[],[],[],{"title":1122,"abstract":1123,"authors":1156,"affiliations":1167,"tags":1168,"publishDate":342,"draft":29,"pdfUrl":1138,"readingTime":64},[1157,1159,1161,1163,1165],{"name":1126,"affiliationIndices":1158},[],{"name":258,"affiliationIndices":1160},[20],{"name":1131,"affiliationIndices":1162},[],{"name":261,"affiliationIndices":1164},[20],{"name":264,"affiliationIndices":1166},[20],[268],[1140,1141,1142,45,1143,1144,1145,1146],[],"limpnet-lightweight-multisensor-navigation-drones",{"id":1170,"data":1172,"filePath":1190,"digest":1191,"rendered":1192},{"title":1173,"abstract":1174,"authors":1175,"affiliations":1182,"pdfUrl":1184,"draft":29,"tags":1185,"publishDate":1189},"LiMPNet: Lightweight Multi-sensor Perception and DRL Navigation for Tiny Drones in Mapless Environments","Autonomous tiny drones face significant challenges in navigation due to strict constraints on size, weight, power, and onboard computational capacity. This paper presents a lightweight navigation framework that integrates basic multisensor perception with deep reinforcement learning (DRL) to enable safe, mapless flight in cluttered environments. We employ the Crazyflie 2.1 nano-drone, equipped with a grayscale camera and a multi-ranger deck, a laser-based distance sensor, for real-time obstacle detection and avoidance. A Proximal Policy Optimization (PPO) agent is trained within a ROS and Gazebo simulation environment to generate collision-free trajectories using fused visual and range data. The system is evaluated in two environments: a simple obstacle field, where the drone achieves a 100% success rate (112/112 episodes), and a densely cluttered map, where it reaches the target in 35% of trials (7/20). These results demonstrate that effective autonomous navigation is achievable using minimal sensing and low-computation models, making it well-suited for resource-constrained aerial platforms.",[1176,1179],{"name":1177,"affiliationIndices":1178},"Omer Kurkutlu",[20],{"name":1180,"affiliationIndices":1181},"Arman Roohi",[20],[1183],"University of Illinois Chicago","https://drive.google.com/file/d/1cU246SMvDQ8XKFyeM1m6voHTrY1osgbi/view?usp=share_link",[1186,1187,325,1188],"drones","reinforcement-learning","sensors",["Date","2025-11-06T11:00:00.000Z"],"src/content/post/limpnet-lightweight-multisensor-navigation-drones.md","cab6a66f4c6a7e61",{"html":50,"metadata":1193},{"headings":1194,"localImagePaths":1195,"remoteImagePaths":1196,"frontmatter":1197,"imagePaths":1205},[],[],[],{"title":1173,"abstract":1174,"authors":1198,"affiliations":1203,"tags":1204,"publishDate":246,"draft":29,"pdfUrl":1184,"readingTime":64},[1199,1201],{"name":1177,"affiliationIndices":1200},[20],{"name":1180,"affiliationIndices":1202},[20],[1183],[1186,1187,325,1188],[],"multi-modal-robotic-touch",{"id":1206,"data":1208,"filePath":1227,"digest":1228,"rendered":1229},{"title":1209,"abstract":1210,"authors":1211,"affiliations":1217,"pdfUrl":1220,"draft":29,"tags":1221,"publishDate":1226},"Multi-Modal Perception and Behavior Adaptation Models for Human State Understanding and Interaction Improvement in Robotic Touch","Robots that can physically interact with humans in a safe, comfortable, and intuitive manner can help in a variety of settings. However, perceptions of the users greatly affect the acceptability of such robots. Ability of the system to understand user’s perception of the physical interaction as well as adapting robot’s behaviors based on user perception and interaction context can facilitate acceptability of these robots. In this paper we propose a perception-based interaction adaptation framework. One main component of this framework is a multi-modal perception model which is grounded on the existing literature and is intended to provide a quantitative estimation of the human state- defined as the perceptions of the physical interaction- by using human, robot, and context information. This model is intended to be comprehensive in many physical Human-Robot Interaction (pHRI) scenarios. The estimated human state is fed to a context-aware behavior adaptation framework which recommends robot behaviors to improve human state using a learned behavior cost model and an optimization formulation. We show the potential and feasibility of such a human state estimation model by evaluating a reduced model, with data collected through a user study. Additionally, through some feature analysis, we aimed to shed light on future interaction designs for pHRI.",[1212,1214],{"name":821,"affiliationIndices":1213},[20],{"name":1215,"affiliationIndices":1216},"Rana Soltani Zarrin",[24],[1218,1219],"The Robotics Institute","Honda Research Institute","https://drive.google.com/file/d/1Q-uPRSets6ctIk6BSHXdowoSnMrPW71_/view",[1222,1223,1224,1225],"touch","physical","multi-modal","improvement",["Date","2025-11-07T11:00:00.000Z"],"src/content/post/multi-modal-robotic-touch.md","0d6de37880d62b15",{"html":50,"metadata":1230},{"headings":1231,"localImagePaths":1232,"remoteImagePaths":1233,"frontmatter":1234,"imagePaths":1242},[],[],[],{"title":1209,"abstract":1210,"authors":1235,"affiliations":1240,"tags":1241,"publishDate":116,"draft":29,"pdfUrl":1220,"readingTime":64},[1236,1238],{"name":821,"affiliationIndices":1237},[20],{"name":1215,"affiliationIndices":1239},[24],[1218,1219],[1222,1223,1224,1225],[],"petri-nets-iterative-development-interactive-robotic-systems",{"id":1243,"data":1245,"filePath":1280,"digest":1281,"rendered":1282},{"title":1246,"abstract":1247,"authors":1248,"affiliations":1263,"pdfUrl":1264,"draft":29,"tags":1265,"publishDate":1279},"Petri Nets for the Iterative Development of Interactive Robotic Systems","In this position paper, we argue for the use of Petri nets as a modeling language for the iterative development process of interactive robotic systems. Our goal is a representation that can unify various phases of the development process — design, specification, simulation, validation, implementation, and deployment. We focus on how Timed Colored Petri nets (TCPNs), in particular, can provide value for the end-to-end development of interactive systems. We additionally discuss future directions for creating a domain-specific variant of TCPNs tailored specifically for HRI systems development.",[1249,1252,1255,1258,1260],{"name":1250,"affiliationIndices":1251},"Pragathi Praveena",[20],{"name":1253,"affiliationIndices":1254},"Andrew Schoen",[20],{"name":1256,"affiliationIndices":1257},"Michael Gleicher",[20],{"name":258,"affiliationIndices":1259},[24],{"name":1261,"affiliationIndices":1262},"Bilge Mutlu",[20],[267,268],"https://drive.google.com/file/d/1WM1704X4o3_fXjYO3dGhMTPgFcCLVM1K/view",[1266,1267,1268,1269,1270,1271,1272,1273,948,1274,1275,1276,1277,684,1278],"petri nets","timed colored petri nets","tcpns","interactive robotic systems","iterative development","modeling language","system design","specification","validation","implementation","deployment","hri systems","end-to-end development",["Date","2023-10-26T14:00:00.000Z"],"src/content/post/petri-nets-iterative-development-interactive-robotic-systems.md","9a1f4899c6c14e33",{"html":50,"metadata":1283},{"headings":1284,"localImagePaths":1285,"remoteImagePaths":1286,"frontmatter":1287,"imagePaths":1305},[],[],[],{"title":1246,"abstract":1247,"authors":1288,"affiliations":1299,"tags":1300,"publishDate":481,"draft":29,"pdfUrl":1264,"readingTime":64},[1289,1291,1293,1295,1297],{"name":1250,"affiliationIndices":1290},[20],{"name":1253,"affiliationIndices":1292},[20],{"name":1256,"affiliationIndices":1294},[20],{"name":258,"affiliationIndices":1296},[24],{"name":1261,"affiliationIndices":1298},[20],[267,268],[1301,1302,1303,1269,1270,1271,1272,1273,948,1274,1275,1276,1304,684,1278],"Petri nets","Timed Colored Petri nets","TCPNs","HRI systems",[],"plan-owl-automated-pddl",{"id":1306,"data":1308,"filePath":1322,"digest":1323,"rendered":1324},{"title":1309,"abstract":1310,"authors":1311,"affiliations":1317,"pdfUrl":1319,"draft":29,"tags":1320,"publishDate":1321},"PlanOwl: Automated PDDL Files Generation from OWL Ontologies and Visual Language Models","Automated task planning traditionally relies on manually generated domain models, creating bottlenecks in scalability and requiring extensive domain expertise. This paper presents a novel framework to automate the process of generating Planning Domain Definition Language (PDDL) domains and problem files by integrating Web Ontology Language (OWL) ontologies with Visual Language Models (VLMs). Our approach leverages the rich semantic structure of OWL ontologies to systematically define domain classes, predicates, and actions, while VLMs ground abstract ontological concepts in concrete visual observations—automating the generation of instance-specific planning problems. The proposed framework transforms ontological knowledge into PDDL domain files through a mapping algorithm that preserves semantic relationships and logical constraints. The VLM performs visual scene analysis to identify relevant objects, attributes, and spatial configurations for generating initial states, while natural language instructions are used to derive goal states. We evaluate the framework across multiple planning domains, demonstrating that it generates syntactically correct and semantically coherent PDDL domain and problem files directly from OWL ontologies, camera images, and natural language inputs. The resulting files are comparable in quality to those manually generated by planning experts and outperform previous automated systems in terms of semantic fidelity and adaptability.",[1312,1314],{"name":993,"affiliationIndices":1313},[20],{"name":1315,"affiliationIndices":1316},"Paolo Forte",[24],[1005,1318],"Orebro University","https://drive.google.com/file/d/1Wypf_HMbWtH0UomwhYjXjNLvDnneVYNS/view",[539,94,1048,532],["Date","2025-11-06T11:00:00.000Z"],"src/content/post/plan-owl-automated-pddl.md","1d9df91b9338148d",{"html":50,"metadata":1325},{"headings":1326,"localImagePaths":1327,"remoteImagePaths":1328,"frontmatter":1329,"imagePaths":1337},[],[],[],{"title":1309,"abstract":1310,"authors":1330,"affiliations":1335,"tags":1336,"publishDate":246,"draft":29,"pdfUrl":1319,"readingTime":64},[1331,1333],{"name":993,"affiliationIndices":1332},[20],{"name":1315,"affiliationIndices":1334},[24],[1005,1318],[539,94,1048,532],[],"pragmatic-temporal-alignment-stateful-generative-ai",{"id":1338,"data":1340,"filePath":1365,"digest":1366,"rendered":1367},{"title":1341,"abstract":1342,"authors":1343,"affiliations":1353,"pdfUrl":1355,"draft":29,"tags":1356,"publishDate":1364},"Towards Pragmatic Temporal Alignment in Stateful Generative AI Systems: A Configurable Approach","Temporal alignment in stateful generative artificial intelligence (AI) systems remains an underexplored area, particularly beyond goal-driven approaches in planning. Stateful refers to maintaining a persistent memory or “state” across runs or sessions. This helps with referencing past information to make system outputs more contextual and relevant. This position paper proposes a framework for temporal alignment with several configurable toggles. We present four alignment mechanisms: knowledge graph path-based, neural score-based, vector similarity-based, and sequential process-guided alignment. By offering these interchangeable approaches, we aim to provide a flexible solution adaptable to complex and real-world applications. This paper discusses the potential benefits and challenges of each alignment method and positions the importance of a configurable system in advancing progress in stateful generative AI systems.",[1344,1347,1350],{"name":1345,"affiliationIndices":1346},"Kaushik Roy",[20],{"name":1348,"affiliationIndices":1349},"Yuxin Zi",[20],{"name":1351,"affiliationIndices":1352},"Amit Sheth",[20],[1354],"Artificial Intelligence Institute, University of South Carolina","https://drive.google.com/file/d/1bMzbeThODCKULMIws3wIs0F_SoR0iiq6/view",[1357,1358,1359,1360,1361,1362,463,1363],"temporal-alignment","stateful-ai","generative-ai","configurable-systems","temporal-consistency","ai-systems","contextual-ai",["Date","2024-11-08T11:00:00.000Z"],"src/content/post/pragmatic-temporal-alignment-stateful-generative-ai.md","0988489c95521479",{"html":50,"metadata":1368},{"headings":1369,"localImagePaths":1370,"remoteImagePaths":1371,"frontmatter":1372,"imagePaths":1382},[],[],[],{"title":1341,"abstract":1342,"authors":1373,"affiliations":1380,"tags":1381,"publishDate":342,"draft":29,"pdfUrl":1355,"readingTime":64},[1374,1376,1378],{"name":1345,"affiliationIndices":1375},[20],{"name":1348,"affiliationIndices":1377},[20],{"name":1351,"affiliationIndices":1379},[20],[1354],[1357,1358,1359,1360,1361,1362,463,1363],[],"siftom-robust-spoken-instruction-following",{"id":1383,"data":1385,"filePath":1420,"digest":1421,"rendered":1422},{"title":1386,"abstract":1387,"authors":1388,"affiliations":1408,"pdfUrl":1412,"draft":29,"tags":1413,"publishDate":1419},"SIFToM: Robust Spoken Instruction Following through Theory of Mind","Spoken language instructions are ubiquitous in agent collaboration. However, in human-robot collaboration, recognition accuracy for human speech is often influenced by various speech and environmental factors, such as background noise, the speaker’s accents, and mispronunciation. When faced with noisy or unfamiliar auditory inputs, humans use context and prior knowledge to disambiguate the stimulus and take pragmatic actions, a process referred to as top-down processing in cognitive science. We present a cognitively inspired model, Speech Instruction Following through Theory of Mind (SIFToM), to enable robots to pragmatically follow human instructions under diverse speech conditions by inferring the human’s goal and joint plan as prior for speech perception and understanding. We test SIFToM in simulated home experiments (VirtualHome 2). Results show that the SIFToM model outperforms state-of-the-art speech and language models, approaching human-level accuracy on challenging speech instruction following tasks. We then demonstrate its ability at the task planning level on a mobile manipulator for breakfast preparation tasks.",[1389,1392,1394,1397,1400,1402,1405],{"name":1390,"affiliationIndices":1391},"Lance Ying",[20,24],{"name":769,"affiliationIndices":1393},[81],{"name":1395,"affiliationIndices":1396},"Shivam Aarya",[85],{"name":1398,"affiliationIndices":1399},"Yizirui Fang",[85],{"name":581,"affiliationIndices":1401},[81],{"name":1403,"affiliationIndices":1404},"Joshua B. Tenenbaum",[20],{"name":1406,"affiliationIndices":1407},"Tianmin Shu",[85],[1409,1410,587,1411],"Massachusetts Institute of Technology","Harvard University","John Hopkins University","https://drive.google.com/file/d/16CNrReCShv8hSlD64L2-GR0mx-oaY96z/view",[1414,1415,784,275,1416,45,786,1417,1418,147],"theory-of-mind","spoken-instruction-following","cognitive-modeling","speech-recognition","pragmatic-reasoning",["Date","2024-11-07T11:00:00.000Z"],"src/content/post/siftom-robust-spoken-instruction-following.md","6dba496aab86e08d",{"html":50,"metadata":1423},{"headings":1424,"localImagePaths":1425,"remoteImagePaths":1426,"frontmatter":1427,"imagePaths":1445},[],[],[],{"title":1386,"abstract":1387,"authors":1428,"affiliations":1443,"tags":1444,"publishDate":669,"draft":29,"pdfUrl":1412,"readingTime":64},[1429,1431,1433,1435,1437,1439,1441],{"name":1390,"affiliationIndices":1430},[20,24],{"name":769,"affiliationIndices":1432},[81],{"name":1395,"affiliationIndices":1434},[85],{"name":1398,"affiliationIndices":1436},[85],{"name":581,"affiliationIndices":1438},[81],{"name":1403,"affiliationIndices":1440},[20],{"name":1406,"affiliationIndices":1442},[85],[1409,1410,587,1411],[1414,1415,784,275,1416,45,786,1417,1418,147],[],"social-robots-genetic-risk-assessment-cancer",{"id":1446,"data":1448,"filePath":1489,"digest":1490,"rendered":1491},{"title":1449,"abstract":1450,"authors":1451,"affiliations":1476,"pdfUrl":1480,"draft":29,"tags":1481,"publishDate":1488},"Using Social Robots and AI to Perform Genetic Risk Assessment for Cancer","Genetic risk assessment (GRA) and genetic counseling have become integral to optimal patient care for patients with cancer. At present, there is a limited number of qualified healthcare providers who provide this service. To assist professionals in the GRA process, we have combined social robotics and retrieval-augmented generative artificial intelligence (RAG AI) to provide education related to hereditary cancer to be included in GRA sessions for individuals at risk. This GRA application pushes the boundary on previously unavailable chatbots and AI systems by creating a novel and interactive experience enhanced by professionally verified information. In the future, we seek to improve the app as it is and obtain feedback from both GRA professionals and potential end-users that will be used to enhance the system and provide customized risk assessment. Overall, our GRA system takes the next step towards informing patients of their hereditary cancer risk and pertinent care options.",[1452,1455,1458,1461,1464,1467,1470,1473],{"name":1453,"affiliationIndices":1454},"Tyler Morris",[20],{"name":1456,"affiliationIndices":1457},"Conor Brown",[20],{"name":1459,"affiliationIndices":1460},"Seungwoo An",[20],{"name":1462,"affiliationIndices":1463},"Jeremiah Augustine",[20],{"name":1465,"affiliationIndices":1466},"Andrew Ward",[20],{"name":1468,"affiliationIndices":1469},"Laura Enomoto",[24,81],{"name":1471,"affiliationIndices":1472},"Erin Campbell",[24,81],{"name":1474,"affiliationIndices":1475},"Xiaopeng Zhao",[20],[1477,1478,1479],"University of Tennessee, Knoxville","University of Tennessee, UT Medical Center","University of Tennessee, Graduate School of Medicine","https://drive.google.com/file/d/1dsjspBrDljUm14E-am2QIyEpzJOvZG82/view",[1482,1483,1484,1485,275,1486,1487],"social-robots","genetic-risk-assessment","cancer","healthcare","medical-applications","ai-healthcare",["Date","2024-11-07T11:00:00.000Z"],"src/content/post/social-robots-genetic-risk-assessment-cancer.md","561f79edc00f6eb0",{"html":50,"metadata":1492},{"headings":1493,"localImagePaths":1494,"remoteImagePaths":1495,"frontmatter":1496,"imagePaths":1516},[],[],[],{"title":1449,"abstract":1450,"authors":1497,"affiliations":1514,"tags":1515,"publishDate":669,"draft":29,"pdfUrl":1480,"readingTime":64},[1498,1500,1502,1504,1506,1508,1510,1512],{"name":1453,"affiliationIndices":1499},[20],{"name":1456,"affiliationIndices":1501},[20],{"name":1459,"affiliationIndices":1503},[20],{"name":1462,"affiliationIndices":1505},[20],{"name":1465,"affiliationIndices":1507},[20],{"name":1468,"affiliationIndices":1509},[24,81],{"name":1471,"affiliationIndices":1511},[24,81],{"name":1474,"affiliationIndices":1513},[20],[1477,1478,1479],[1482,1483,1484,1485,275,1486,1487],[],"supporting-ai-planning-collaboration-robotic-applications-taems",{"id":1517,"data":1519,"filePath":1556,"digest":1557,"rendered":1558},{"title":1520,"abstract":1521,"authors":1522,"affiliations":1535,"pdfUrl":1539,"draft":29,"tags":1540,"publishDate":1555},"Supporting AI Planning and Collaboration for Robotic Applications Using TAEMS","This paper describes a general approach to integrating higher-level reasoning mechanisms including planning and scheduling methods with lower-level robotic control processes. We adopt a domain-independent task representation language TAEMS to describe the knowledge of tasks, resources, and their interrelationships. This TAEMS representation language serves as the input of the reasoning functions, which generate a schedule of executable methods to be executed by the robot in the physical world. In the execution process of this goal-directed plan, the robot also needs to attend to basic functions. The potential interactions between the plan and these basic functions would lead to interesting challenges that will be discussed. An integrated development platform with a simulator that supports real-world physics is also presented.",[1523,1526,1529,1532],{"name":1524,"affiliationIndices":1525},"Shelley Zhang",[20,24],{"name":1527,"affiliationIndices":1528},"Alexander Moulton",[24],{"name":1530,"affiliationIndices":1531},"Abhijot Bedi",[24],{"name":1533,"affiliationIndices":1534},"Eugene Chabot",[81],[1536,1537,1538],"Gordon College","University of Massachusetts Dartmouth","NUWC Division Newport","https://drive.google.com/file/d/1CBVneEHmC_tn0zxhQGrC8Mx6xSk_KVlz/view",[1541,1542,1543,1544,1545,1546,1547,1548,948,1549,1550,1551,1552,1553,1554],"ai planning","taems","task representation","scheduling","robotic control","reasoning","goal-directed planning","robot execution","integrated development","collaboration","planning and scheduling","domain-independent","knowledge representation","real-world physics",["Date","2023-10-26T11:00:00.000Z"],"src/content/post/supporting-ai-planning-collaboration-robotic-applications-taems.md","345a1bf8953b3ff3",{"html":50,"metadata":1559},{"headings":1560,"localImagePaths":1561,"remoteImagePaths":1562,"frontmatter":1563,"imagePaths":1577},[],[],[],{"title":1520,"abstract":1521,"authors":1564,"affiliations":1573,"tags":1574,"publishDate":557,"draft":29,"pdfUrl":1539,"readingTime":64},[1565,1567,1569,1571],{"name":1524,"affiliationIndices":1566},[20,24],{"name":1527,"affiliationIndices":1568},[24],{"name":1530,"affiliationIndices":1570},[24],{"name":1533,"affiliationIndices":1572},[81],[1536,1537,1538],[1575,1576,1543,1544,1545,1546,1547,1548,948,1549,1550,1551,1552,1553,1554],"AI planning","TAEMS",[],"statistical-agent-based-model",{"id":1578,"data":1580,"filePath":1594,"digest":1595,"rendered":1596},{"title":1581,"abstract":1582,"authors":1583,"affiliations":1587,"pdfUrl":1589,"draft":29,"tags":1590,"publishDate":1593},"A Statistical Agent-Based Model Of Development And Evaluation","This paper describes a prospective, preference-driven, practical model that may have useful systems engineering applications. This paper begins by describing the model’s mindbody-environment (MBE) conceptual foundation and continues with an overview of how each MBE component can be represented. In brief, the model assumes that a system can be assembled/composed from constituent parts with appropriate tools. An environment can be represented by the set of all possible composition options. A mind can be represented by a network of parts and tools animated by algorithms that guide composition. A body can be represented as that portion of the environment that a mind can directly control to affect other portions of the environment. The practical model is a software implementation of these representations. Finally, this paper describes the further work needed to advance the model.",[1584],{"name":1585,"affiliationIndices":1586},"Samuel Denard",[20],[1588],"Empirical Products & Services","https://drive.google.com/file/d/1Pf_W8Zi0_yaQuXsfZ1QfOgEE4PsF4W0I/view",[539,532,1591,1592],"agent-based modeling","algorithms",["Date","2025-11-06T11:00:00.000Z"],"src/content/post/statistical-agent-based-model.md","cc41ca862a0dc636",{"html":50,"metadata":1597},{"headings":1598,"localImagePaths":1599,"remoteImagePaths":1600,"frontmatter":1601,"imagePaths":1607},[],[],[],{"title":1581,"abstract":1582,"authors":1602,"affiliations":1605,"tags":1606,"publishDate":246,"draft":29,"pdfUrl":1589,"readingTime":64},[1603],{"name":1585,"affiliationIndices":1604},[20],[1588],[539,532,1591,1592],[],"statewise-petri-net-visual-editor-robotic-systems",{"id":1608,"data":1610,"filePath":1636,"digest":1637,"rendered":1638},{"title":1611,"abstract":1612,"authors":1613,"affiliations":1622,"pdfUrl":1625,"draft":29,"tags":1626,"publishDate":1635},"Statewise: A Petri Net-Based Visual Editor for Specifying Robotic Systems","We present Statewise, a visual editor designed to enable developers to model and simulate complex systems using colored Petri nets in an intuitive, graphical way. Utilizing Statewise, we explore two use cases to demonstrate its capabilities. We also discuss potential enhancements to further extend its applicability in more complex scenarios.",[1614,1617,1620],{"name":1615,"affiliationIndices":1616},"Zejun Zhou",[20],{"name":1618,"affiliationIndices":1619},"Yuchen Jin",[24],{"name":1250,"affiliationIndices":1621},[81],[1623,1624,830],"Department of Computer Science, Brown University","Department of Computer Sciences, University of Wisconsin–Madison","https://drive.google.com/file/d/1zBnBbJyca1NZvsw9bM1Wgi5aJGOdFhB-/view",[1627,1628,1629,1630,1631,789,45,1632,1633,1634],"petri-nets","visual-editor","robotic-systems","system-specification","modeling","software-tools","concurrent-systems","verification",["Date","2024-11-08T16:00:00.000Z"],"src/content/post/statewise-petri-net-visual-editor-robotic-systems.md","c9a32ccb41bd66e8",{"html":50,"metadata":1639},{"headings":1640,"localImagePaths":1641,"remoteImagePaths":1642,"frontmatter":1643,"imagePaths":1653},[],[],[],{"title":1611,"abstract":1612,"authors":1644,"affiliations":1651,"tags":1652,"publishDate":514,"draft":29,"pdfUrl":1625,"readingTime":64},[1645,1647,1649],{"name":1615,"affiliationIndices":1646},[20],{"name":1618,"affiliationIndices":1648},[24],{"name":1250,"affiliationIndices":1650},[81],[1623,1624,830],[1627,1628,1629,1630,1631,789,45,1632,1633,1634],[],"survey-robotic-language-grounding-symbols-embeddings",{"id":1654,"data":1656,"filePath":1681,"digest":1682,"rendered":1683},{"title":1657,"abstract":1658,"authors":1659,"affiliations":1673,"pdfUrl":1675,"draft":29,"tags":1676,"publishDate":1680},"A Survey of Robotic Language Grounding: Tradeoffs between Symbols and Embeddings","With large language models, robots can understand language more flexibly and more capable than ever before. This survey reviews and situates recent literature into a spectrum with two poles: 1) mapping between language and some manually defined formal representation of meaning, and 2) mapping between language and high-dimensional vector spaces that translate directly to low-level robot policy. Using a formal representation allows the meaning of the language to be precisely represented, limits the size of the learning problem, and leads to a framework for interpretability and formal safety guarantees. Methods that embed language and perceptual data into high-dimensional spaces avoid this manually specified symbolic structure and thus have the potential to be more general when fed enough data but require more data and computing to train. We discuss the benefits and tradeoffs of each approach and finish by providing directions for future work that achieves the best of both worlds.",[1660,1663,1665,1668,1670],{"name":1661,"affiliationIndices":1662},"Vanya Cohen",[20],{"name":769,"affiliationIndices":1664},[24],{"name":1666,"affiliationIndices":1667},"Raymond Mooney",[20],{"name":581,"affiliationIndices":1669},[24,81],{"name":1671,"affiliationIndices":1672},"David Watkins",[81],[941,587,1674],"The AI Institute","https://drive.google.com/file/d/1lbDF3jKypjlhYFHja9DGIr6Hixg83q_s/view",[786,1677,1678,784,45,1679,320,275],"symbolic-reasoning","neural-embeddings","survey",["Date","2024-11-07T16:00:00.000Z"],"src/content/post/survey-robotic-language-grounding-symbols-embeddings.md","cabaf45d86ae7614",{"html":50,"metadata":1684},{"headings":1685,"localImagePaths":1686,"remoteImagePaths":1687,"frontmatter":1688,"imagePaths":1702},[],[],[],{"title":1657,"abstract":1658,"authors":1689,"affiliations":1700,"tags":1701,"publishDate":63,"draft":29,"pdfUrl":1675,"readingTime":64},[1690,1692,1694,1696,1698],{"name":1661,"affiliationIndices":1691},[20],{"name":769,"affiliationIndices":1693},[24],{"name":1666,"affiliationIndices":1695},[20],{"name":581,"affiliationIndices":1697},[24,81],{"name":1671,"affiliationIndices":1699},[81],[941,587,1674],[786,1677,1678,784,45,1679,320,275],[],"unified-formalism-constructing-embodied-agents",{"id":1703,"data":1705,"filePath":1738,"digest":1739,"rendered":1740},{"title":1706,"abstract":1707,"authors":1708,"affiliations":1718,"pdfUrl":1722,"draft":29,"tags":1723,"publishDate":1737},"A Unified Formalism for Constructing Embodied Agents","This paper reviews PUG, a cognitive architecture for embodied agents, with a focus on its formalism for representing expertise. This includes a rule-like notation for encoding concepts that describe states, motives that compute utilities, skills that calculate control values to achieve goals, and processes that predict changes to the environment. In each case, we review the syntax for modular knowledge elements and how the architecture uses them to generate dynamic content. We also discuss how the framework integrates them to produce teleoreactive behavior over time.",[1709,1712,1715],{"name":1710,"affiliationIndices":1711},"Pat Langley",[20],{"name":1713,"affiliationIndices":1714},"Edward Katz",[24],{"name":1716,"affiliationIndices":1717},"Mohan Sridharan",[81],[1719,1720,1721],"Institute for the Study of Learning and Expertise","Stanford Intelligent Systems Laboratory","University of Birmingham","https://drive.google.com/file/d/1M5dPsYmH5atl-9BwFL6BXcau1P4yTqdc/view",[1724,1725,1726,1727,1553,1728,1729,1730,1731,1732,1733,1734,1735,1736],"cognitive architecture","embodied agents","pug","formalism","rule-based systems","teleoreactive behavior","agent architecture","expertise representation","modular knowledge","dynamic content","goal-oriented behavior","environment modeling","unified formalism",["Date","2023-10-25T11:00:00.000Z"],"src/content/post/unified-formalism-constructing-embodied-agents.md","5ae04d60b31af43f",{"html":50,"metadata":1741},{"headings":1742,"localImagePaths":1743,"remoteImagePaths":1744,"frontmatter":1745,"imagePaths":1756},[],[],[],{"title":1706,"abstract":1707,"authors":1746,"affiliations":1753,"tags":1754,"publishDate":440,"draft":29,"pdfUrl":1722,"readingTime":64},[1747,1749,1751],{"name":1710,"affiliationIndices":1748},[20],{"name":1713,"affiliationIndices":1750},[24],{"name":1716,"affiliationIndices":1752},[81],[1719,1720,1721],[1724,1725,1755,1727,1553,1728,1729,1730,1731,1732,1733,1734,1735,1736],"PUG",[],"trade-middleware-advanced-robotics",{"id":1757,"data":1759,"filePath":1773,"digest":1774,"rendered":1775},{"title":1760,"abstract":1761,"authors":1762,"affiliations":1766,"pdfUrl":1767,"draft":29,"tags":1768,"publishDate":1772},"The TRADE Middleware for Advanced Robotic Architectures","Over the last decade, the Robot Operating System (ROS) has become the de facto standard for robotic middleware having significantly improved some of the shortcomings of version 1 with the release of version 2. Yet, while the focus of ROS 2 has been “downward” on the underlying communication layer, the interfaces “upward” to the robotic architectures implemented in ROS has received little attention. In this paper, we argue that robotic middleware can serve important roles for robotic architectures, in particular, cognitive robotic architecture, if the right kinds of interfaces are provided that allow for a tight integration between architecture and middleware. We introduce the Thinking Robots Agent Development Environment, TRADE, which is an extension of the previous Agent Development Environment, ADE, and provides advanced features for architecture integration and interactions between cognitive robotic architecture and the middleware layer. We describe several features in TRADE that are missing in ROS, in particular, system-wide locking mechanisms, service instrumentation, and middleware service calls and discuss how they can support the architecture developers with implementing advanced architectural features such as dialogue-based system debugging and configuration, or multi-effector multi-robot behavior coordination.",[1763],{"name":1764,"affiliationIndices":1765},"Matthias Scheutz",[20],[90],"https://drive.google.com/file/d/1XJnh8DWebOUyA_eLpb_4kS5CS8Jp5_qC/view",[1769,1770,1771,419],"middleware","architecture","debugging",["Date","2025-11-06T11:00:00.000Z"],"src/content/post/trade-middleware-advanced-robotics.md","50e31759587778c4",{"html":50,"metadata":1776},{"headings":1777,"localImagePaths":1778,"remoteImagePaths":1779,"frontmatter":1780,"imagePaths":1786},[],[],[],{"title":1760,"abstract":1761,"authors":1781,"affiliations":1784,"tags":1785,"publishDate":246,"draft":29,"pdfUrl":1767,"readingTime":64},[1782],{"name":1764,"affiliationIndices":1783},[20],[90],[1769,1770,1771,419],[],"verifiable-toolchain-robotics",{"id":1787,"data":1789,"filePath":1871,"digest":1872,"rendered":1873},{"title":1790,"abstract":1791,"authors":1792,"affiliations":1854,"pdfUrl":1861,"draft":29,"tags":1862,"publishDate":1870},"Towards a Verifiable Toolchain for Robotics","There is a growing need for autonomous robots to complete complex tasks robustly in dynamic and unstructured environments. However, current robot performance is limited to simple tasks in controlled environments. To improve robot autonomy in complex environments, the robot’s deliberation system must be able to synthesise correct plans for a task and generate contingency plans for handling anomalous scenarios that were not expected at design time. The robustness of such a system can be quantified using techniques for formal verification and validation. This paper outlines the progress of EU project CONVINCE (CONtext-aware Verifiable and adaptIve dyNamiC dEliberation), which aims to develop a software toolchain that aids developers in designing, developing, and deploying robot deliberation systems that are fully verified. We describe our modelling approach, each of the toolchain components, and how they interact. We also discuss survey results which demonstrate the demand for a verifiable toolchain among the robotics community.",[1793,1796,1799,1802,1805,1808,1811,1814,1817,1821,1824,1827,1830,1833,1836,1839,1842,1845,1848,1851],{"name":1794,"affiliationIndices":1795},"Charlie Street",[20],{"name":1797,"affiliationIndices":1798},"Yazz Warsame",[20],{"name":1800,"affiliationIndices":1801},"Masoumeh Mansouri",[20],{"name":1803,"affiliationIndices":1804},"Michaela Klauck",[24],{"name":1806,"affiliationIndices":1807},"Christian Henkel",[24],{"name":1809,"affiliationIndices":1810},"Marco Lampacrescia",[24],{"name":1812,"affiliationIndices":1813},"Matteo Palmas",[24,81],{"name":1815,"affiliationIndices":1816},"Ralph Lange",[24],{"name":1818,"affiliationIndices":1819},"Enrico Ghiorzi",[81,1820],5,{"name":1822,"affiliationIndices":1823},"Armando Tacchella",[81],{"name":1825,"affiliationIndices":1826},"Razane Azrou",[85],{"name":1828,"affiliationIndices":1829},"Raphaël Lallement",[85],{"name":1831,"affiliationIndices":1832},"Matteo Morelli",[85],{"name":1834,"affiliationIndices":1835},"Ginny I. Chen",[929],{"name":1837,"affiliationIndices":1838},"Danielle Wallis",[929],{"name":1840,"affiliationIndices":1841},"Stefano Bernagozzi",[81,1820],{"name":1843,"affiliationIndices":1844},"Stefano Rosa",[1820],{"name":1846,"affiliationIndices":1847},"Marco Randazzo",[1820],{"name":1849,"affiliationIndices":1850},"Sofia Faraci",[1820],{"name":1852,"affiliationIndices":1853},"Lorenzo Natale",[1820],[1855,1856,1857,1858,1859,1860],"University of Birmingham, UK","Robert Bosch GmbH, Bosch Research, Germany","Università degli Studi di Genova, Italy","List, CEA, Université Paris-Saclay, France","Inventya Ventures, Ireland","Istituto Italiano di Tecnologia, Italy","https://drive.google.com/file/d/1LV1AJfQDkzGCQlOYtf1puRsCucUXFANC/view",[1863,1864,1865,1866,1867,1868,1869,1629],"formal-verification","robotics-toolchain","safety","reliability","software-engineering","automated-testing","verification-methods",["Date","2024-11-08T11:00:00.000Z"],"src/content/post/verifiable-toolchain-robotics.md","e4f8124fd0b849da",{"html":50,"metadata":1874},{"headings":1875,"localImagePaths":1876,"remoteImagePaths":1877,"frontmatter":1878,"imagePaths":1922},[],[],[],{"title":1790,"abstract":1791,"authors":1879,"affiliations":1920,"tags":1921,"publishDate":342,"draft":29,"pdfUrl":1861,"readingTime":64},[1880,1882,1884,1886,1888,1890,1892,1894,1896,1898,1900,1902,1904,1906,1908,1910,1912,1914,1916,1918],{"name":1794,"affiliationIndices":1881},[20],{"name":1797,"affiliationIndices":1883},[20],{"name":1800,"affiliationIndices":1885},[20],{"name":1803,"affiliationIndices":1887},[24],{"name":1806,"affiliationIndices":1889},[24],{"name":1809,"affiliationIndices":1891},[24],{"name":1812,"affiliationIndices":1893},[24,81],{"name":1815,"affiliationIndices":1895},[24],{"name":1818,"affiliationIndices":1897},[81,1820],{"name":1822,"affiliationIndices":1899},[81],{"name":1825,"affiliationIndices":1901},[85],{"name":1828,"affiliationIndices":1903},[85],{"name":1831,"affiliationIndices":1905},[85],{"name":1834,"affiliationIndices":1907},[929],{"name":1837,"affiliationIndices":1909},[929],{"name":1840,"affiliationIndices":1911},[81,1820],{"name":1843,"affiliationIndices":1913},[1820],{"name":1846,"affiliationIndices":1915},[1820],{"name":1849,"affiliationIndices":1917},[1820],{"name":1852,"affiliationIndices":1919},[1820],[1855,1856,1857,1858,1859,1860],[1863,1864,1865,1866,1867,1868,1869,1629],[],"unified-representation-social-robot-actions",{"id":1923,"data":1925,"filePath":1947,"digest":1948,"rendered":1949},{"title":1926,"abstract":1927,"authors":1928,"affiliations":1935,"pdfUrl":1938,"draft":29,"tags":1939,"publishDate":1946},"Agreeing to Disagree: Translating Representations to Uncover a Unified Representation for Social Robot Actions","Researchers and designers of social robots often approach robot control system design from a single perspective; such as designing autonomous robots, teleoperated robots, or robots programmed by an end-user. While each design approach presents a tradeoff between some advantages and limitations, there is an opportunity to integrate these approaches where people benefit from the best-fit approach for their use case. In this work, we propose integrating these seemingly distinct robot control approaches to uncover a common data representation of social actions defining social expression by a robot. We demonstrate the value of integrating an authoring system, teleoperation interface, and robot planning system by integrating instances of these systems for robot storytelling. By relying on an integrated system, domain experts can define behaviors through end-user interfaces that teleoperators and autonomous robot programmers can use directly thus providing a cohesive expert-driven robot system.",[1929,1932],{"name":1930,"affiliationIndices":1931},"Saad Elbeleidy",[20],{"name":1933,"affiliationIndices":1934},"Jason Wilson",[24],[1936,1937],"Peerbots","Franklin & Marshall College","https://drive.google.com/file/d/1yhaiqx8xg7Jw3izscUIhqVWj8ZJrlR7u/view",[1940,320,1941,1942,1943,1944,275,1945],"social-robotics","unified-representations","robot-actions","translation-methods","social-behavior-modeling","interoperability",["Date","2024-11-08T16:00:00.000Z"],"src/content/post/unified-representation-social-robot-actions.md","caaaa2bc0ed0749f",{"html":50,"metadata":1950},{"headings":1951,"localImagePaths":1952,"remoteImagePaths":1953,"frontmatter":1954,"imagePaths":1962},[],[],[],{"title":1926,"abstract":1927,"authors":1955,"affiliations":1960,"tags":1961,"publishDate":514,"draft":29,"pdfUrl":1938,"readingTime":64},[1956,1958],{"name":1930,"affiliationIndices":1957},[20],{"name":1933,"affiliationIndices":1959},[24],[1936,1937],[1940,320,1941,1942,1943,1944,275,1945],[],"vizij-design-animation-deployment-faces",{"id":1963,"data":1965,"filePath":1996,"digest":1997,"rendered":1998},{"title":1966,"abstract":1967,"authors":1968,"affiliations":1988,"pdfUrl":1990,"draft":29,"tags":1991,"publishDate":1995},"Vizij: Supporting the design, animation, and deployment of rendered robot faces","The lack of a standardized, open-source platform for expressive, rendered robot faces is hindering effective human-robot interaction (HRI). To address this challenge, we present a solution under development, Vizij , an open-source ecosystem of tools that provide a pipeline for building, animating, sharing, and deploying rendered robot faces. Unlike a “blank canvas”, Vizij defines a standardized-yet-modular, user-informed rigging and controller system—including for eye gaze, visemes, and emotional expression—that offers robust communicative capabilities out-of-the-box. In this paper, we present the system requirements, architecture, and modules within Vizij , define preliminary standards for this system and discuss example interfaces and use cases that can be built and supported with this system.",[1969,1971,1973,1976,1979,1982,1985],{"name":1930,"affiliationIndices":1970},[20],{"name":1253,"affiliationIndices":1972},[24],{"name":1974,"affiliationIndices":1975},"Chris Birmingham",[24],{"name":1977,"affiliationIndices":1978},"Tiago Ribeiro",[24],{"name":1980,"affiliationIndices":1981},"Victor Paléologue",[24],{"name":1983,"affiliationIndices":1984},"Douglas Dooley",[24],{"name":1986,"affiliationIndices":1987},"Ross Mead",[24],[1936,1989],"Semio","https://drive.google.com/file/d/1z9b5e4G7aoGKQ9KI39IsKefg5IKhh_Xd/view",[1992,1993,1994,1276],"faces","design","animation",["Date","2025-11-07T11:00:00.000Z"],"src/content/post/vizij-design-animation-deployment-faces.md","29e6b4f12579e9ab",{"html":50,"metadata":1999},{"headings":2000,"localImagePaths":2001,"remoteImagePaths":2002,"frontmatter":2003,"imagePaths":2021},[],[],[],{"title":1966,"abstract":1967,"authors":2004,"affiliations":2019,"tags":2020,"publishDate":116,"draft":29,"pdfUrl":1990,"readingTime":64},[2005,2007,2009,2011,2013,2015,2017],{"name":1930,"affiliationIndices":2006},[20],{"name":1253,"affiliationIndices":2008},[24],{"name":1974,"affiliationIndices":2010},[24],{"name":1977,"affiliationIndices":2012},[24],{"name":1980,"affiliationIndices":2014},[24],{"name":1983,"affiliationIndices":2016},[24],{"name":1986,"affiliationIndices":2018},[24],[1936,1989],[1992,1993,1994,1276],[],"series",["Map",2024,2025,2039,2040],"citrus-docs",{"id":2024,"data":2026,"filePath":2030,"digest":2031,"rendered":2032},{"id":2024,"title":2027,"description":2028,"featured":2029},"Citrus Docs","Astro Citrus documentation outlines key aspects of the template, describing its core functionality for blog management and project documentation setup",true,"src/content/series/citrus-docs.md","9587a82953734c4c",{"html":50,"metadata":2033},{"headings":2034,"localImagePaths":2035,"remoteImagePaths":2036,"frontmatter":2037,"imagePaths":2038},[],[],[],{"id":2024,"title":2027,"description":2028,"featured":2029,"readingTime":64},[],"markdown-elements",{"id":2039,"data":2041,"filePath":2044,"digest":2045,"rendered":2046},{"id":2039,"title":2042,"description":2043,"featured":29},"Markdown Elements","Dive into a comprehensive guide exploring Markdown syntax and elements, from basic formatting to advanced features, designed to help you master their usage with practical examples for enhancing your documentation and writing efficiency","src/content/series/markdown-elements.md","6fb3819652d108bb",{"html":50,"metadata":2047},{"headings":2048,"localImagePaths":2049,"remoteImagePaths":2050,"frontmatter":2051,"imagePaths":2052},[],[],[],{"id":2039,"title":2042,"description":2043,"readingTime":64},[]]